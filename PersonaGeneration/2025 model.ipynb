{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18d5ca1b-dc1a-4829-ba1c-f0f5626c2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle(f\"newsarticles/newsarticles2020-07-01.pkl\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a186dc1f-23ee-44d9-b745-2273fcdd6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai \n",
    "import time \n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import logging\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262feb6-bd97-448d-9055-52118032f2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8b630388-0390-4fce-af21-6acb58447226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Social Media Simulator\n"
     ]
    }
   ],
   "source": [
    "# Universal parameters\n",
    "\n",
    "openai.api_key = #######\n",
    "pd.set_option('display.max_columns', None)  \n",
    "\n",
    "PERSPECTIVE_API_KEY = 'AIzaSyAxTjb4F0tKxk-X6_s3Nd5E1VHKbok8KuU'\n",
    "\n",
    "logging.basicConfig(filename='error_log.txt', level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.warning(\"Running Social Media Simulator\")\n",
    "\n",
    "# 'gpt-4'\n",
    "MODEL = 'gpt-3.5-turbo' #This can be overrun\n",
    "WAIT_TIME = 0.1\n",
    "\n",
    "LLM_TEMPERATURE = 0.8\n",
    "LLM_PRESENCE_PENALTY = 1.0\n",
    "\n",
    "\n",
    "\n",
    "# Decides the preloaded newsfile that is used.\n",
    "# Cutoff for GPT3.5 is June 2020. For GPT4 it's september 2021.\n",
    "NEWS_DATE = \"2020-07-01\" \n",
    "\n",
    "REMOVE_HASHTAGS_FROM_REPLIES = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb9df674-3600-4a8b-9c53-db4fe66e3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=PERSPECTIVE_API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "049c245d-5907-4f4e-8de0-12b7debc1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for generating GPT strings from ANES \n",
    "def format_list(words):\n",
    "    if len(words) == 0:\n",
    "        return \"\"\n",
    "    elif len(words) == 1:\n",
    "        return words[0]\n",
    "    else:\n",
    "        formatted_words = \", \".join(words[:-1])\n",
    "        return f\"{formatted_words}, and {words[-1]}\"\n",
    "    \n",
    "# This code fetches lines from the ANES, and returns as readable dict\n",
    "def get_anes_rows(number_rows):\n",
    "\n",
    "    df = pd.read_csv('anes_timeseries_2020_csv_20220210.csv', low_memory=False)\n",
    "\n",
    "    df1 = df\n",
    "    # V202545 how often post polituical content on twitter?\n",
    "    # V202544 how often do you use twitter? \n",
    "\n",
    "    ##  Rename columns\n",
    "    col_recode = {'V201600':'gender',\n",
    "    'V203000': 'state',\n",
    "    'V201511x': 'education',\n",
    "    'V201534x':'employed',\n",
    "    'V201549x':'race',\n",
    "    'V201601':'sexOrientation',\n",
    "    'V201602':'justifiedViolence',\n",
    "    'V201617x': 'income',\n",
    "    'V201627': 'selfCensor',\n",
    "    'V201628':'gunsOwned',\n",
    "    'V201005':'attentionPolitics',\n",
    "    'V202073':'vote2020',\n",
    "    'V201103':'vote2016',\n",
    "    'V201105':'vote2012',\n",
    "    'V201116':'afraid',\n",
    "    'V201117':'outraged',\n",
    "    'V201118': 'angry',\n",
    "    'V201119':'happy',\n",
    "    'V201120':'worried',\n",
    "    'V201121':'proud',\n",
    "    'V201122':'irritated',\n",
    "    'V201123':'nervous',\n",
    "    'V201201':'liberalOrConservative',\n",
    "    'V201228':'partyIdentif',\n",
    "    'V201231x':'strongIdentif',\n",
    "    'V201232':'partyIdentity',\n",
    "    'V201156':'feelingDemocratic',\n",
    "    'V201157':'feelingRepublican',\n",
    "    'V202544':'howOftenUseTwitter'}\n",
    "\n",
    "    df1 = df1.rename(columns=col_recode)\n",
    "\n",
    "    #Copy for use later\n",
    "    df1['V201231x'] = df1['strongIdentif'] \n",
    "\n",
    "    ## Recode values\n",
    "    dic = {-9: None,\n",
    "           1: 'male',\n",
    "           2:'female'}\n",
    "\n",
    "    df1 = df1.replace({'gender': dic})\n",
    "\n",
    "\n",
    "    dic = {1: 'Alabama',\n",
    "    2: 'Alaska',\n",
    "    4: 'Arizona',   \n",
    "    5: 'Arkansas',\n",
    "    6: 'California',\n",
    "    8: 'Colorado',\n",
    "    9: 'Connecticut',\n",
    "    10: 'Delaware',\n",
    "    11: 'Washington DC',\n",
    "    12: 'Florida',\n",
    "    13: 'Georgia',\n",
    "    15: 'Hawaii',\n",
    "    16: 'Idaho',\n",
    "    17: 'Illinois',\n",
    "    18: 'Indiana',\n",
    "    19: 'Iowa',\n",
    "    20: 'Kansas',\n",
    "    21: 'Kentucky',\n",
    "    22: 'Louisiana',\n",
    "    23: 'Maine',\n",
    "    24: 'Maryland',\n",
    "    25: 'Massachusetts',\n",
    "    26: 'Michigan',\n",
    "    27: 'Minnesota',\n",
    "    28: 'Mississippi',\n",
    "    29: 'Missouri',\n",
    "    30: 'Montana',\n",
    "    31: 'Nebraska',\n",
    "    32: 'Nevada',\n",
    "    33: 'New Hampshire',\n",
    "    34: 'New Jersey',\n",
    "    35: 'New Mexico',\n",
    "    36: 'New York',\n",
    "    37: 'North Carolina',\n",
    "    38: 'North Dakota',\n",
    "    39: 'Ohio',\n",
    "    40: 'Oklahoma',\n",
    "    41: 'Oregon',\n",
    "    42: 'Pennsylvania',\n",
    "    44: 'Rhode Island',\n",
    "    45: 'South Carolina',\n",
    "    46: 'South Dakota',\n",
    "    47: 'Tennessee',\n",
    "    48: 'Texas',\n",
    "    49: 'Utah',\n",
    "    50: 'Vermont',\n",
    "    51: 'Virginia',\n",
    "    53: 'Washington',\n",
    "    54: 'West Virginia',\n",
    "    55: 'Wisconsin',\n",
    "    56: 'Wyoming'}\n",
    "\n",
    "    df1 = df1.replace({'state': dic})\n",
    "\n",
    "\n",
    "    dic = {-9: None,\n",
    "           -8: None,\n",
    "           -2: None,\n",
    "          1: \"Less than high school\",\n",
    "          2: \"High school\",\n",
    "          3: \"High school\",\n",
    "          4: \"Bachelorâ€™s degree\",\n",
    "          5: \"Graduate degree\"}\n",
    "\n",
    "    df1 = df1.replace({'education': dic})\n",
    "\n",
    "\n",
    "    dic = {-2: None,\n",
    "           1: 'employed',\n",
    "           2: 'unemployed',\n",
    "           4: 'unemployed',\n",
    "           5: 'unemployed',\n",
    "           6: 'unemployed',\n",
    "           7: 'unemployed',\n",
    "           8: 'unemployed'} \n",
    "\n",
    "    df1 = df1.replace({'employed': dic})\n",
    "\n",
    "    dic = {1: 'White',\n",
    "           2: 'Black',\n",
    "           3: 'Hispanic',\n",
    "           4: 'Asian',\n",
    "           5: 'Native American',\n",
    "           6: 'Multiple races',\n",
    "          -9: None,\n",
    "           -8: None} \n",
    "\n",
    "    df1 = df1.replace({'race': dic})\n",
    "\n",
    "    dic = {1: 'heterosexual',\n",
    "           2: 'homosexual',\n",
    "           3: 'bisexual',\n",
    "           4: None,\n",
    "          -9: None,\n",
    "           -5: None} \n",
    "\n",
    "    df1 = df1.replace({'sexOrientation': dic})\n",
    "\n",
    "\n",
    "#     dic = {1: 'Family income below 30k',\n",
    "#            2: 'Family income below 30k',\n",
    "#            3: 'Family income below 30k',\n",
    "#            4: 'Family income below 30k',\n",
    "#            5: 'Family income below 30k',\n",
    "#            6: 'Family income 30-60k',\n",
    "#            7: 'Family income 30-60k',\n",
    "#            8: 'Family income 30-60k',\n",
    "#            9: 'Family income 30-60k',\n",
    "#            10: 'Family income 30-60k',\n",
    "#            11: 'Family income 60-90k',\n",
    "#            12: 'Family income 60-90k',\n",
    "#            13: 'Family income 60-90k',\n",
    "#            14: 'Family income 60-90k',\n",
    "#            15: 'Family income 60-90k',\n",
    "#            16: 'Family income more 90-175k',\n",
    "#            17: 'Family income more 90-175k',\n",
    "#            18: 'Family income more 90-175k',\n",
    "#            19: 'Family income more 90-175k',\n",
    "#            20: 'Family income more 90-175k',\n",
    "#            21: 'Family income more 175k',\n",
    "#            22: 'Family income more 175k',\n",
    "#            -9: None,\n",
    "#            -5: None} \n",
    "    \n",
    "#     df1 = df1.replace({'income': dic})\n",
    "\n",
    "    dic = {-9: None,\n",
    "           -5: None,\n",
    "          1:'I never or rarely stop myself from saying something because I think someone might call me a racist, a sexist, or otherwise a bad person',\n",
    "          2: 'I never or rarely stop myself from saying something because I think someone might call me a racist, a sexist, or otherwise a bad person',\n",
    "           3: 'I occasionally stop myself from saying something because I think someone might call me a racist, a sexist, or otherwise a bad person',\n",
    "           4: 'I often stop myself from saying something because I think someone might call me a racist, a sexist, or otherwise a bad person',\n",
    "           5: 'I often stop myself from saying something because I think someone might call me a racist, a sexist, or otherwise a bad person'} \n",
    "\n",
    "    df1 = df1.replace({'selfCensor': dic})\n",
    "\n",
    "\n",
    "    dic = {-9: None,-5: None}\n",
    "\n",
    "    df1 = df1.replace({'gunsOwned': dic})\n",
    "\n",
    "    dic = {-9: None,\n",
    "           1: \"I always or most of the time pay attention to what's going on in government and politics\",\n",
    "           2: \"I always or most of the time pay attention to what's going on in government and politics\",\n",
    "           3: \"I pay attention to what's going on in government and politics about half the time\",\n",
    "           4: \"I pay attention to what's going on in government and politics never or some of the time\",\n",
    "           5: \"I pay attention to what's going on in government and politics never or some of the time\"}\n",
    "\n",
    "    df1 = df1.replace({'attentionPolitics': dic})\n",
    "\n",
    "\n",
    "    dic = {-9: None,\n",
    "           -8: None,\n",
    "           -7: None,\n",
    "           -1: \"Didn't vote\",\n",
    "           1: 'Joe Biden',\n",
    "           2: 'Donald Trump',\n",
    "           3: None,\n",
    "           4: None,\n",
    "           5: None,\n",
    "           -6: None,\n",
    "           7: 'Donald Trump',\n",
    "           8: None,\n",
    "           11: None,\n",
    "           12: None,}\n",
    "\n",
    "    df1 = df1.replace({'vote2020': dic})\n",
    "\n",
    "\n",
    "    dic = {-9: None,\n",
    "           -8:None,\n",
    "           -1:'Didn\\t vote',\n",
    "           1: 'Hillary Clinton',\n",
    "           2: 'Donald Trump',\n",
    "           5:  None}\n",
    "\n",
    "    df1 = df1.replace({'vote2016': dic})\n",
    "\n",
    "    dic = {-9: None,\n",
    "           -8:None,\n",
    "           -1:'Didn\\t vote',\n",
    "           1: 'Barack Obama',\n",
    "           2: 'Mitt Romney',\n",
    "           5:  None}\n",
    "\n",
    "    df1 = df1.replace({'vote2012': dic})\n",
    "\n",
    "\n",
    "    dic = {-9: None,\n",
    "           -8: None,\n",
    "           -4: None,\n",
    "           -1: None,\n",
    "           1: 'I consider myself a liberal',\n",
    "           2: 'I consider myself a conservative',\n",
    "           3: None}\n",
    "\n",
    "    df1 = df1.replace({'liberalOrConservative': dic})\n",
    "\n",
    "    df1['party'] = df1['partyIdentif']\n",
    "    dic = {-9: 'Not sure',\n",
    "           -8: 'Not sure',\n",
    "           -4: 'Not sure',\n",
    "           0: 'Not sure',\n",
    "           1: 'Democrat',\n",
    "           2: 'Republican',\n",
    "           3: 'Independent',\n",
    "          5: 'Not sure'}\n",
    "\n",
    "    df1 = df1.replace({'party': dic})\n",
    "\n",
    "    dic = {-9: None,\n",
    "           -8: None,\n",
    "           -4: None,\n",
    "           0: None,\n",
    "           1: 'Democrat',\n",
    "           2: 'Republican',\n",
    "           3: 'Independent',\n",
    "          5: None}\n",
    "\n",
    "    df1 = df1.replace({'partyIdentif': dic})\n",
    "\n",
    "    dic = {-9: None,\n",
    "           -8: None,\n",
    "           1: 'Strong Democrat',\n",
    "           2: 'Democrat',\n",
    "           3: 'Independent who leans Democrat',\n",
    "           4: 'Independent',\n",
    "          5: 'Independent who leans Republican',\n",
    "          6: 'Republican',\n",
    "          7: 'Strong Republican'}\n",
    "\n",
    "    df1 = df1.replace({'strongIdentif': dic})\n",
    "\n",
    "    dic = {-9: None,\n",
    "           -8: None,\n",
    "           -1: None,\n",
    "           1: 'My party is very important to my identity',\n",
    "           2: 'My party is very important to my identity',\n",
    "           3: 'My party is moderately important to my identity',\n",
    "           4: 'My party is not important to my identity',\n",
    "           5: 'My party is not important to my identity'}\n",
    "\n",
    "    df1 = df1.replace({'partyIdentity': dic})\n",
    "\n",
    "\n",
    "    # We select only people who ever use twitter\n",
    "    df1 = df1.loc[df1['howOftenUseTwitter'].isin((1,2,3,4,5,6))]\n",
    "    \n",
    "    #Remove the very small nr of people who did not answer to political affiliation\n",
    "    # df1 = df1.loc[df1['V201231x']>0]\n",
    "    \n",
    "    #Calculate partisanship    \n",
    "    # Remove the small number of individuals who did not respond to partisan feeling temp\n",
    "    df1 = df1.loc[(df1['feelingDemocratic']>=0) & (df1['feelingDemocratic']<=100) ]\n",
    "    df1 = df1.loc[(df1['feelingRepublican']>=0) & (df1['feelingRepublican']<=100) ] \n",
    "    # We use the temperature responses for party, to focus on identity and get a -1,1 value for every individual\n",
    "    df1['partisan'] = (df1['feelingRepublican'] - df1['feelingDemocratic'])/100\n",
    "    \n",
    "    ## Function that generates N random people, with WEIGHTING based on the ANES weighting    \n",
    "    # Note that replacement is key here!\n",
    "    random_rows = df1.sample(n=number_rows,weights = df1['V200010b'], replace=True) if number_rows is not None else df1\n",
    "    \n",
    "    random_dicts = random_rows.to_dict(orient=\"records\")\n",
    "\n",
    "    #These are the codes for media channels\n",
    "    # V201634a yahoo.com\n",
    "    # V201634b cnn.com\n",
    "    # V201634c huffingpost.com \n",
    "    # V201634d nytimes.com\n",
    "    # V201634e breitbart.com\n",
    "    # V201634f foxnews.com\n",
    "    # V201634g washingtonpost.com\n",
    "    # V201634h theguardian.com\n",
    "    # V201634i usatoday.com \n",
    "    # V201634j bbc.com \n",
    "    # V201634k npr.org\n",
    "    # V201634m dailycaller.com\n",
    "    # V201634n bloomberg.com\n",
    "    # V201634p buzzfeed.com\n",
    "    # V201634q nbcnews.com\n",
    "\n",
    "    #Here we produce the persona description\n",
    "    \n",
    "    res = []\n",
    "    for d in random_dicts:\n",
    "        l = {}\n",
    "        #media sources\n",
    "        media = ['V201634a','V201634b','V201634c','V201634d','V201634e','V201634f','V201634g','V201634h','V201634i','V201634j','V201634k','V201634m','V201634n','V201634p']\n",
    "        l['media'] = [m for m in media if d[m]==1]\n",
    "        \n",
    "        l['feelingDemocratic'] = d['feelingDemocratic']\n",
    "        l['feelingRepublican'] = d['feelingRepublican']\n",
    "        # Normalize the twitter use variable: \n",
    "        #NormalizeL: How many times per every second week?\n",
    "        # 1. Many times every day: 50\n",
    "        # 2. A few times every day: 30\n",
    "        # 3. About once a day: 14\n",
    "        # 4. A few times each week: 5\n",
    "        # 5. About once a week: 2\n",
    "        # 6. Once or twice a month: 1\n",
    "        l['howOftenUseTwitter'] = {1:50, 2:30, 3:14, 4: 5, 5:2, 6:1}[d['howOftenUseTwitter']]\n",
    "        \n",
    "        # In your spare time, you like to watch\n",
    "        hobbies = {'V201631a':'American Idol','V201630r':'NCIS','V201631i':'Good Morning America','V201631r':'Saturday Night Live','V201632c':'Amor Eterno','V201633e':'The Dave Ramsey Show'}\n",
    "        hobbies_liked = [v for k,v in hobbies.items() if d[k]==1]\n",
    "\n",
    "        # V201631a PRE: MENTION: TV PROG - AMERICAN IDOL (ABC)\n",
    "        # V201630r PRE: MENTION: TV PROG - NCIS (CBS)\n",
    "        # V201631i PRE: MENTION: TV PROG - GOOD MORNING AMERICA (ABC)\n",
    "        # V201631r PRE: MENTION: TV PROG - SATURDAY NIGHT LIVE (NBC)\n",
    "        # V201632c PRE: MENTION: TV PROG - AMOR ETERNO\n",
    "        # V201633e PRE: MENTION: RADIO PROG - THE DAVE RAMSEY SHOW\n",
    "\n",
    "\n",
    "        l['partisan'] = d['partisan']\n",
    "        \n",
    "        # 4 or 5\n",
    "        feelings = [k for k in ['afraid','outraged','angry','happy','worried','proud','irritated','nervous'] if d[k]==3 or d[k]==4 or d[k]==5]\n",
    "        \n",
    "        # ['V201151']\n",
    "        \n",
    "        \n",
    "        \n",
    "#     dic = {1: 'Family income below 30k',\n",
    "#            2: 'Family income below 30k',\n",
    "#            3: 'Family income below 30k',\n",
    "#            4: 'Family income below 30k',\n",
    "#            5: 'Family income below 30k',\n",
    "#            6: 'Family income 30-60k',\n",
    "#            7: 'Family income 30-60k',\n",
    "#            8: 'Family income 30-60k',\n",
    "#            9: 'Family income 30-60k',\n",
    "#            10: 'Family income 30-60k',\n",
    "#            11: 'Family income 60-90k',\n",
    "#            12: 'Family income 60-90k',\n",
    "#            13: 'Family income 60-90k',\n",
    "#            14: 'Family income 60-90k',\n",
    "#            15: 'Family income 60-90k',\n",
    "#            16: 'Family income more 90-175k',\n",
    "#            17: 'Family income more 90-175k',\n",
    "#            18: 'Family income more 90-175k',\n",
    "#            19: 'Family income more 90-175k',\n",
    "#            20: 'Family income more 90-175k',\n",
    "#            21: 'Family income more 175k',\n",
    "#            22: 'Family income more 175k',\n",
    "#            -9: None,\n",
    "#            -5: None} \n",
    "        \n",
    "        \n",
    "        temps = {\n",
    "            'feelingDemocratic':'Democrats',\n",
    "            'feelingRepublican':'Republicans',\n",
    "            'V201151': 'Joe Biden',\n",
    "            'V201152': 'Donald Trump',\n",
    "            'V202168': 'Muslims',\n",
    "            'V202169': 'Christians',\n",
    "            'V202170': 'Jews',\n",
    "            'V202171': 'Police',\n",
    "            'V202172': 'transgender people',\n",
    "            'V202173': 'scientists',\n",
    "            'V202174': 'Black Lives Matter',\n",
    "            'V202175': 'journalists',\n",
    "            'V202178': 'NRA',\n",
    "            'V202184': 'rural Americans',        \n",
    "            'V202158': 'Anthony Fauci',\n",
    "            'V202159': 'Christian Fundamentalists',\n",
    "            'V202160': 'feminists',\n",
    "            'V202161': 'liberals',\n",
    "            'V202164': 'conservatives',\n",
    "            'V202166': 'homosexuals'}\n",
    "        \n",
    "        lovelist = []\n",
    "        hatelist = []\n",
    "        for k,v in temps.items():\n",
    "            if d[k] >= 0 and d[k] <= 10:\n",
    "                hatelist.append(v)\n",
    "            if d[k] <= 100 and d[k] >= 90:\n",
    "                lovelist.append(v)\n",
    "        \n",
    "        #Create persona string\n",
    "        l['persona'] = \"Here is a description of your persona: \\n\"\n",
    "\n",
    "        if d['gender'] is not None:\n",
    "            l['persona'] += f\"You are {d['gender']}.\\n\"\n",
    "            \n",
    "            \n",
    "        if d['income'] is not None and d['income']>0:\n",
    "            if d['income'] >= 1 and d['income'] <= 10:\n",
    "                incomeclass = 'low income'\n",
    "            elif d['income'] >= 11 and d['income'] <= 20:\n",
    "                incomeclass = 'middle income'\n",
    "            else:\n",
    "                incomeclass = 'high income'\n",
    "            l['persona'] += f\"You are {incomeclass}.\\n\"\n",
    "        \n",
    "            \n",
    "        if d['V201507x'] is not None:\n",
    "            l['persona'] += f\"Age: {d['V201507x']}.\\n\" \n",
    "        \n",
    "        religions = {1: 'Protestant', 2: 'Evangelical Protestant', 3: 'Black Protestant', 4: 'Protestant',  5: 'Catholic', 6: 'Christian', 7: 'Jewish', 9: 'not religious'}\n",
    "        l['V201458x'] = d['V201458x']\n",
    "        if d['V201458x'] in list(religions.keys()):\n",
    "            l['persona'] += f\"You are {religions[d['V201458x']]}.\\n\"\n",
    "        \n",
    "        if d['state'] is not None:\n",
    "            l['persona'] += f\"You are from {d['state']}.\\n\"\n",
    "        if d['education'] is not None: \n",
    "            l['persona'] += f\"Education: {d['education']}.\\n\"\n",
    "        # if d['employed'] is not None: \n",
    "            # l['persona'] += f\"You are {d['employed']}.\\n\"\n",
    "        if d['race'] is not None: \n",
    "            l['persona'] += f\"You are {d['race']}.\\n\"\n",
    "        if d['sexOrientation'] is not None: \n",
    "            l['persona'] += f\"You are {d['sexOrientation']}.\\n\"\n",
    "\n",
    "        l['race'] = d['race'] \n",
    "        \n",
    "        l['party'] = 'Democrat' if l['partisan'] < 0 else 'Republican' if l['partisan'] > 0 else 'Non-partisan'        \n",
    "        \n",
    "        #People who never talk about politics: we add other preferences\n",
    "        if d['V202545'] == 5: # V202023\n",
    "            l['persona'] += \"You never talk about politics.\\n\" #\n",
    "            l['never_talk_politics'] = True\n",
    "            \n",
    "            l['party'] = 'Non-partisan'\n",
    "            l['partisan'] = 0\n",
    "            \n",
    "            #Fishing\n",
    "            if d['V202567'] == 1:\n",
    "                l['persona'] += \"You like to go fishing or hunting.\\n\" #\n",
    "                               \n",
    "        #They do talk about politics \n",
    "        else:\n",
    "            l['never_talk_politics'] = False\n",
    "            \n",
    "            if d['vote2020'] in ['Donald Trump','Joe Biden']:\n",
    "                l['persona'] += f\"You voted for {d['vote2020']} in 2020.\\n\"\n",
    "            else:\n",
    "                l['persona'] += \"You didn't vote in 2020.\\n\"\n",
    "                \n",
    "                \n",
    "            # OLD WAY OF ASSIGNING PARTY\n",
    "            # l['party'] = 'Democrat' if d['V201231x'] in [1,2,3] else 'Republican' if d['V201231x'] in [5,6,7] else 'Independent' if d['V201231x'] == 4 else 'None'\n",
    "            \n",
    "            \n",
    "            # if d['strongIdentif'] is not None: \n",
    "            #     l['persona'] += f\"You are a {d['strongIdentif']}.\\n\"\n",
    "        \n",
    "            #This worked poorly\n",
    "            # l['persona'] += f\"On a scale from -100 to 100, where -100 is extremely Democratic and 100 is extremely Republican, you are: {d['partisan']}.\\n\"\n",
    "        \n",
    "            \n",
    "            #Generate party affiliation\n",
    "            if l['partisan'] == 0:\n",
    "                l['persona'] += 'You prefer neither political party.\\n'\n",
    "            if l['partisan'] < 0 and l['partisan'] > -0.2:\n",
    "                l['persona'] += 'You prefer the Democrats.\\n'\n",
    "            if l['partisan'] > 0 and l['partisan'] < 0.2:\n",
    "                l['persona'] += 'You prefer the Republicans.\\n'\n",
    "            if l['partisan'] <= -0.2 and l['partisan'] > -0.5:\n",
    "                l['persona'] += 'You are a Democrat.\\n'\n",
    "            if l['partisan'] >= 0.2 and l['partisan'] < 0.5:\n",
    "                l['persona'] += 'You are a Republican.\\n'\n",
    "            if l['partisan'] <= -0.5:\n",
    "                l['persona'] += 'You are a strong Democrat.\\n'\n",
    "            if l['partisan'] >= 0.5:\n",
    "                l['persona'] += 'You are a strong Republican.\\n'           \n",
    "            \n",
    "            if d['justifiedViolence'] in [3,4,5]:\n",
    "                l['persona'] += \"You think political violence is justified.\\n\"\n",
    "\n",
    "            if len(lovelist)>0:\n",
    "                l['persona'] += f'You love {format_list(lovelist)}.\\n'\n",
    "\n",
    "            if len(hatelist)>0:\n",
    "                l['persona'] += f'You hate {format_list(hatelist)}.\\n'            \n",
    "\n",
    "        #You post online a lot or always about politics, or you get into political argument in the last 12 months\n",
    "        if d['V202545']== 1 or d['V202545']== 2: #d['V202024']== 1 or \n",
    "            l['persona'] += \"You like to argue about politics.\\n\"\n",
    "            \n",
    "        \n",
    "        if len(feelings)>0:\n",
    "            l['persona'] += f\"You feel {format_list(feelings)} about your country.\\n\"\n",
    "\n",
    "        if len(hobbies_liked)>0:\n",
    "            l['persona'] += f'You like to watch {format_list(hobbies_liked)} on TV.\\n'\n",
    "\n",
    "        l['attribs'] = {key: d[key] for key in ['gender','state','education','feelingDemocratic','feelingRepublican','V202023','race','strongIdentif']}\n",
    "        l['attribs']['hobbies_liked'] = hobbies_liked\n",
    "        l['attribs']['media'] = l['media']\n",
    "        \n",
    "        res.append(l)\n",
    "\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73dfaf2a-a0bc-4fde-840a-acdc7602c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # anes = get_anes_rows(500)\n",
    "# prompt = whats_your_name()\n",
    "# for i in range(len(anes)):\n",
    "#     if 'name' in anes[i]:\n",
    "#         continue\n",
    "#     print(i)\n",
    "#     name = gpt(anes[i]['persona'],prompt)\n",
    "#     anes[i]['name'] = name\n",
    "\n",
    "# with open('anes_preloaded.pkl','wb') as f:\n",
    "#     pickle.dump(anes,f)\n",
    "# print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d5a42ae-27d0-4f46-b989-fee9a9acf53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 500\n",
    "# anes = get_anes_rows(n)\n",
    "# prompt = whats_your_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4af73f4b-84e2-4b9d-8def-7195325d5806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get_anes_agents(100)\n",
    "# for i in range(len(anes)):\n",
    "#     if 'name' not in anes[i]:\n",
    "#         print(i)\n",
    "#         name = gpt(anes[i]['persona'],prompt)\n",
    "#         anes[i]['name'] = name.strip(\"'\\\". \")\n",
    "\n",
    "# with open('anes_preloaded.pkl','wb') as f:\n",
    "#     pickle.dump(anes,f)\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58871d2d-ee60-49c3-8a6f-9c962c90db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name(s):\n",
    "    pattern = r\"Name: (.+?)\\n\"\n",
    "\n",
    "    # Use re.search() with the DOTALL flag to match across lines\n",
    "    match = re.search(pattern, s, re.DOTALL)\n",
    "\n",
    "    # Check if a match was found\n",
    "    # if match:\n",
    "    name = match.group(1).strip()\n",
    "    return name\n",
    "\n",
    "# This preloads agents and stores them on file to prevent having to invent new names every time\n",
    "# We load 500 agents, and give them names, and store them.\n",
    "def preload_agent_personality_fill(n=500):\n",
    "    anes = get_anes_rows(n)\n",
    "    for i in range(len(anes)):\n",
    "        print(i,end=', ')\n",
    "        if 'name' not in anes[i]:\n",
    "            prompt = personality_filler_prompt(anes[i]['persona'])    \n",
    "            extended = gpt_no_persona(prompt) \n",
    "\n",
    "            extended = extended.replace(\"Additional attributes:\", '').replace(\"Additional personality traits:\", '').replace(\"New attributes:\", '').replace(\"Additional personality attributes:\", '').replace(\"New personality attributes:\", '')\n",
    "            print(prompt)\n",
    "            print(extended)\n",
    "            print()\n",
    "            anes[i]['extended_persona'] = extended.strip(\"'\\\". \")        \n",
    "            anes[i]['name'] = parse_name(extended)\n",
    "\n",
    "    with open('anes_preloaded_personality.pkl','wb') as f:\n",
    "        pickle.dump(anes,f)\n",
    "    print(\"Done\")\n",
    "\n",
    "# anes = get_anes_rows(n)\n",
    "\n",
    "\n",
    "def get_anes_agents(n):\n",
    "    anes = None\n",
    "    with open('anes_preloaded_personality.pkl','rb') as f:\n",
    "        anes = pickle.load(f)\n",
    "    if n > len(anes):\n",
    "        raise ValueError('Did not preload enough ANES rows to handle this many requests.')\n",
    "    \n",
    "    #The sample is already pre-weighted\n",
    "    return random.sample(anes,k=n)\n",
    "\n",
    "#Run this to carry out preloading. \n",
    "# preload_agent_names(600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de10b3-96a8-4341-96b3-a5b53382dd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74670bf7-2a54-42f8-a8a7-5b10ea251b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "# The following distribution was identifying by analyzing the distribution of the number of words of 300,000 tweets from the US. \n",
    "def get_random_tweet_length():\n",
    "    return round(stats.lognorm.rvs(s=0.8065686222594033, loc=0.21776116140300664, scale=10.686281912527175))\n",
    "\n",
    "\n",
    "# # This preloads agents and stores them on file to prevent having to invent new names every time\n",
    "# # We load 500 agents, and give them names, and store them.\n",
    "# def preload_agent_names(n=500):\n",
    "#     anes = get_anes_rows(n)\n",
    "#     prompt = whats_your_name()\n",
    "#     for i in range(len(anes)):\n",
    "#         name = gpt(anes[i]['persona'],prompt)\n",
    "#         anes[i]['name'] = name.strip(\"'\\\". \")\n",
    "    \n",
    "#     with open('anes_preloaded.pkl','wb') as f:\n",
    "#         pickle.dump(anes,f)\n",
    "#     print(\"Done\")\n",
    "\n",
    "# def get_anes_agents(n):\n",
    "#     anes = None\n",
    "#     with open('anes_preloaded.pkl','rb') as f:\n",
    "#         anes = pickle.load(f)\n",
    "#     if n > len(anes):\n",
    "#         raise ValueError('Did not preload enough ANES rows to handle this many requests.')\n",
    "    \n",
    "#     #The sample is already pre-weighted\n",
    "#     return random.sample(anes,k=n)\n",
    "\n",
    "# #Run this to carry out preloading. \n",
    "# # preload_agent_names(600)\n",
    "\n",
    "\n",
    "# Call ChatGPT\n",
    "def gpt(persona, message,model=MODEL,wait_time=WAIT_TIME):\n",
    " \n",
    "    logging.info(f\"GPT processing.\\n Persona: {persona}. \\n Message: {message}\")\n",
    "  \n",
    "    fail = 0\n",
    "    while True:\n",
    "        try:\n",
    "\n",
    "            # INSTRUCTION\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                temperature=LLM_TEMPERATURE,\n",
    "                presence_penalty = LLM_PRESENCE_PENALTY,\n",
    "                messages=([{'role':'system','content':persona},\n",
    "                           {'role':'user','content':message}]),\n",
    "                request_timeout=60\n",
    "            )\n",
    "            result = \"\"\n",
    "            for choice in response.choices:\n",
    "                result += choice.message.content\n",
    "\n",
    "            time.sleep(wait_time)\n",
    "            logging.info(f\"GPT response: '{result}'\")\n",
    "\n",
    "            return result\n",
    "        \n",
    "        #If it's too long, we simply cut the string until it's a good length and give a warning\n",
    "        # Not ideal, but signals that the model is too small. No other solution is great either.\n",
    "        \n",
    "        #If the text is too long, we truncate it and try again. Note that if you get this error, you probably want to chunk your texts.\n",
    "        except openai.error.InvalidRequestError as e:\n",
    "            #Shorten request text\n",
    "            logging.warning(f\"Received a InvalidRequestError. Request likely too long; cutting 10% of the text and trying again. {e}\")\n",
    "            time.sleep(5)\n",
    "            words = message.split()\n",
    "            num_words_to_remove = round(len(words) * 0.1)\n",
    "            remaining_words = words[:-num_words_to_remove]\n",
    "            message = ' '.join(remaining_words)\n",
    "            failed = True\n",
    "        \n",
    "        except Exception as e:\n",
    "            fail += 1 \n",
    "            logging.error(f\"Caught exception while trying GPT: {e}\")\n",
    "            time.sleep(5)\n",
    "            if fail > 5:\n",
    "                return ''\n",
    "\n",
    "def gpt_no_persona(message,model=MODEL,wait_time=WAIT_TIME):\n",
    " \n",
    "    logging.info(f\"GPT processing. No persona. \\n Message: {message}\")\n",
    "  \n",
    "    fail = 0\n",
    "    while True:\n",
    "        try:\n",
    "\n",
    "            # INSTRUCTION\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                temperature=LLM_TEMPERATURE,\n",
    "                presence_penalty = LLM_PRESENCE_PENALTY,\n",
    "                messages=([{'role':'user','content':message}]),\n",
    "                request_timeout=60\n",
    "            )\n",
    "            result = \"\"\n",
    "            for choice in response.choices:\n",
    "                result += choice.message.content\n",
    "\n",
    "            time.sleep(wait_time)\n",
    "            logging.info(f\"GPT response: '{result}'\")\n",
    "\n",
    "            return result\n",
    "        \n",
    "        #If it's too long, we simply cut the string until it's a good length and give a warning\n",
    "        # Not ideal, but signals that the model is too small. No other solution is great either.\n",
    "        \n",
    "        #If the text is too long, we truncate it and try again. Note that if you get this error, you probably want to chunk your texts.\n",
    "        except openai.error.InvalidRequestError as e:\n",
    "            #Shorten request text\n",
    "            logging.warning(f\"Received a InvalidRequestError. Request likely too long; cutting 10% of the text and trying again. {e}\")\n",
    "            time.sleep(5)\n",
    "            words = message.split()\n",
    "            num_words_to_remove = round(len(words) * 0.1)\n",
    "            remaining_words = words[:-num_words_to_remove]\n",
    "            message = ' '.join(remaining_words)\n",
    "            failed = True\n",
    "        \n",
    "        except Exception as e:\n",
    "            fail += 1 \n",
    "            logging.error(f\"Caught exception while trying GPT: {e}\")\n",
    "            time.sleep(5)\n",
    "            if fail > 5:\n",
    "                return ''            \n",
    "            \n",
    "            \n",
    "# This takes a string with a list of actions returned from GPT and parses it to a dict\n",
    "def parse_action_string(string):\n",
    "    string = string.strip()\n",
    "    #We can't trust GPT3.5 to not add linebreaks even when we tell it not to.\n",
    "    string = string.replace(\"\\n\", '. ')\n",
    "    string = string.replace(\". .\", '. ')\n",
    "    string = string.replace(\"..\", '. ')\n",
    "    logging.info(f\"Parsing action string: {string}\")\n",
    "    actions = {}\n",
    "    items = string.split('. ')\n",
    "    for i in range(0, len(items), 2):        \n",
    "        number = int(items[i].strip('. '))\n",
    "        action = items[i + 1]\n",
    "        actions[number] = action\n",
    "    return actions\n",
    "\n",
    "#This parses the response from ChatGPT to respond to a message.\n",
    "#These are having to deal with ChatGPT3.5 being extremely unruly when it comes to generating according to specified formatting.\n",
    "def parse_respond_to_message(response):\n",
    "    \n",
    "    if \"AI model\" in response or \"AI language model\" in response:\n",
    "        logging.warning(\"Model returned 'As an AI model', skipping this response...\")\n",
    "        return None,None\n",
    "    \n",
    "    #Get the final line\n",
    "    response = response.strip(\"'\\\"\").strip()\n",
    "    final_line = response.split('\\n')[-1]\n",
    "    \n",
    "    i = None\n",
    "    r = final_line.split(' ',1)\n",
    "    \n",
    "    #Find the headline id. 1 means that it splits max 2 (for some reason)\n",
    "    try:\n",
    "        #Maybe it tried responding to a response.\n",
    "        #If so, we just treat it as a response to the thread by rounding off downward.\n",
    "        i = int(float(r[0]))-1\n",
    "        message = r[1].strip(\"'\\\"\").strip()\n",
    "    \n",
    "    #Maybe it didn't repeat the number on the last line. \n",
    "    #If so, maybe it gave the number on the first line\n",
    "    #Let's try.\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            rr = response.split(' ',1)\n",
    "            i = int(float(rr[0]))-1\n",
    "        except Exception:\n",
    "            try:\n",
    "                #Sometimes, rarely, the entire first line is the number\n",
    "                rr = response.split('\\n',1)\n",
    "                i = int(float(rr[0].strip()))-1\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed parsing message response. {e}. Response: {response}.\")\n",
    "                return None,None\n",
    "                \n",
    "        #If so, the entire final line is likely the message\n",
    "        message = final_line \n",
    "        \n",
    "    #Sometimes gpt will say their name before message. \n",
    "    message = message.split(\":\",1)[-1].strip('\"\\' ')\n",
    "    \n",
    "    if REMOVE_HASHTAGS_FROM_REPLIES:\n",
    "        message = re.sub(r'#\\w+', '', message)\n",
    "    \n",
    "    return i,message\n",
    "\n",
    "\n",
    "\n",
    "def parse_first_message(message):\n",
    "    \n",
    "    \n",
    "    if \"AI model\" in message or \"AI language model\" in message:\n",
    "        logging.warning(\"Model returned 'As an AI model', skipping this response...\")\n",
    "        return None,None\n",
    "    \n",
    "    message = message.strip(\"'\\\"\").strip()\n",
    "        \n",
    "    #Find the headline id\n",
    "    r = message.split('.',1)\n",
    "    \n",
    "    i = None\n",
    "    try:\n",
    "        i = int(r[0])-1\n",
    "    except ValueError:\n",
    "        try:\n",
    "            r = message.split(':',1)\n",
    "            i = int(r[0])-1\n",
    "        except ValueError:\n",
    "            r = message.split('-',1)\n",
    "            i = int(r[0].strip())-1\n",
    "\n",
    "    #If the response contains a line-break, ChatGPT has probably ignored our instructions and repeated the headline.\n",
    "    #We split on the linebreak and take the second part\n",
    "    comment = None\n",
    "    if \"\\n\" in message:\n",
    "        comment = message.split('\\n')[-1]\n",
    "    elif \"Message: \" in message:\n",
    "        comment = message.split('Message: ')[1]\n",
    "    else:\n",
    "        #Parse response\n",
    "        comment = r[-1].strip()\n",
    "\n",
    "    # Sometimes gpt3 randomly repeats the id for the next line. Sometimes in creative ways. We deal with these.\n",
    "    pattern = r'^(?:\\d+\\.|#\\d+\\s*-*|Number \\d+:\\s*|\\d+\\s*-*|Message:|)\\s*'\n",
    "\n",
    "    # Use re.sub() to remove the matched pattern from the input string\n",
    "    comment = re.sub(pattern, '', comment)\n",
    "    comment = comment.strip(\"'\\\" []\")\n",
    "    \n",
    "    #Occassionally, they will repeat the headline once again, with a :\n",
    "    comment = comment.split(':')[-1].strip()\n",
    "    \n",
    "    return i, comment\n",
    "\n",
    "\n",
    "# Each node has an attribute partisanship in (-1,1). \n",
    "# The probability of two nodes i,j being connected should be proportional to (1 - abs(p_i - p_j)/2)**H where H is the homophily parameter. \n",
    "#  The each node should have num_connections edges.\n",
    "# Note that the algorithm prioritizes the number of connections over maintaining homophily. \n",
    "def generate_homophilous_network(partisan, num_connections, homophily_parameter, names):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    num_nodes = len(partisan)\n",
    "    G.add_nodes_from([(i, {'partisan': partisan[i], 'label': names[i]}) for i in range(num_nodes)])\n",
    "    nodes = list(G.nodes())\n",
    "    \n",
    "    # For each node, list all \n",
    "    for i in range(num_nodes):\n",
    "        pi = G.nodes[i]['partisan']\n",
    "        #Probability of a connection is proportional to 1 minus distance in partisanship to the power of the homophily parameter, unless they're the same node or already connected\n",
    "        probabilities = [(1 - abs(pi - G.nodes[j]['partisan']) / 2) ** homophily_parameter if i!=j and not G.has_edge(i, j) and not G.has_edge(j, i) else 0 for j in range(num_nodes)]\n",
    "        #Since connections = outgoing + ingoing, we need to divide the num_connections by 2\n",
    "        # nredges = int(num_connections/2)\n",
    "        if num_connections > len(probabilities):\n",
    "            logging.warning(\"Generating network: too few nodes to reach the target edge count!\")\n",
    "        neighbors = random.choices(list(range(num_nodes)),weights=probabilities, k=min(num_connections, len(probabilities)))\n",
    "        G.add_edges_from([(i, j) for j in neighbors])\n",
    "    \n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e73aaae-2b4d-45d9-945a-2aac88eef613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def whats_your_name():\n",
    "#     return \"\"\"Based on your persona, pick a random plausible name for yourself. [Respond only with a first name and a surname. Don't motivate your answer. Do not use quotation marks.]\"\"\"\n",
    "\n",
    "\n",
    "def personality_filler_prompt(persona):\n",
    "    return f\"\"\"You will get a description of a person. Your task is to add other plausible personality traits that fits the described person, such as hobbies, favorite sports teams, specific political opinions, or other personality attributes. Give the person a name and a surname. [Respond with the new attributes. Use concise language and respond briefly. Only list the traits, without saying e.g. \"additional personality traits\" or describing the task.]\n",
    "    \n",
    "{persona}\"\"\"\n",
    "\n",
    "\n",
    "def get_headlines(nr_headlines, media_sources):\n",
    "    \n",
    "    # df = pd.read_pickle(\"newsarticles20211021.pkl\")\n",
    "    df = pd.read_pickle(f\"newsarticles/newsarticles{NEWS_DATE}.pkl\")\n",
    "    \n",
    "    #If the user does not like any specific media channels in their ANES survey, we just take headlines from ALL sources.\n",
    "    # Another option would be for them to write something non-news related.\n",
    "    if len(media_sources) > 0:\n",
    "    \n",
    "        missing = set(media_sources) - set(df.key.unique())\n",
    "\n",
    "        if len(missing)>0:\n",
    "            raise Exception(f\"{missing} not included.\")\n",
    "\n",
    "        df = df.loc[df.key.isin(set(media_sources))]\n",
    "\n",
    "    headlines = df.sample(n=min(nr_headlines,len(df)))[['title','media_name','body']].T.to_dict().values()\n",
    "    \n",
    "    #Get only the first N words of news body. Do not include line breaks.\n",
    "    for d in headlines:\n",
    "        d['body'] = ' '.join(d['body'].split()[:150])+'...'\n",
    "    \n",
    "    return list(headlines)\n",
    "\n",
    "\n",
    "\n",
    "# Prompt for the bots first message\n",
    "def first_message_prompt(headlines_text):\n",
    "\n",
    "    #First message\n",
    "    return f\"\"\"Here follows a list of headlines from the newspaper. \n",
    "\n",
    "\"{headlines_text}\"\n",
    "\n",
    "Choose exactly one of these headlines to share on your social media feed based on your persona, and write a comment of 10-50 words about that one headline in the style of your personality. [Write the number of the news item you choose, followed by your message. Do not repeat the headline. Do not use quotation marks. Do not introduce yourself. Use informal language. Do not start your message by describing your persona. Don't use hashtags.]\"\"\"\n",
    "\n",
    "def timeline_to_string(df,include_user_information=False, maximum_comments_to_show=20):\n",
    "    if len(df)==0:\n",
    "        raise ValueError(\"The timeline is empty for the user. This will likely lead to hallucinations.\")\n",
    "        \n",
    "    result = \"\"\n",
    "    # for index, row in df.iterrows():\n",
    "    for index, content, likes, media_source, headline, responses, name, from_party, republican_likes, democrat_likes, nonpartisan_likes in zip(range(len(df)), df.content, df.likes, df.media_source, df.headline, df.responses, df.from_name, df.from_party, df.republican_likes, df.democrat_likes, df.nonpartisan_likes):\n",
    "        result += \"\\n\"\n",
    "        result += f\"{index+1}. {name}{'['+from_party+']' if include_user_information else ''} shares {media_source} article: '{headline}': \\n '{content.strip()}' ({len(likes)} users liked this message.{'[republican likes: '+str(republican_likes) + '. democrat likes: ' + str(democrat_likes) + '. nonpartisan likes: ' + str(nonpartisan_likes) + ']' if include_user_information else ''})\\n\"\n",
    "\n",
    "        if responses is not None and len(responses)>0:\n",
    "            result += \"COMMENTS:\\n\"\n",
    "            if maximum_comments_to_show > 0 and len(responses) > maximum_comments_to_show:\n",
    "                responses = responses[-maximum_comments_to_show:]\n",
    "\n",
    "            for i, reply in enumerate(responses):\n",
    "                result += f\"{index+1}.{i+1} {reply['from_name']}{'['+reply['from_party']+']' if include_user_information else ''}: {reply['content'].strip()}\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def respond_to_message_prompt(timeline, maximum_comments_to_show):\n",
    "\n",
    "    word_length = get_random_tweet_length()\n",
    "    \n",
    "    result = timeline_to_string(timeline,maximum_comments_to_show=maximum_comments_to_show)\n",
    "    \n",
    "    #Reaction\n",
    "    return f'''You are using a social media platform, and see the following message threads: \n",
    "    \n",
    "{result}\n",
    "\n",
    "Choose exactly one of these posts / message threads that your persona is likely to react to emotionally, either because you strongly agree or because your strongly disagree. Engage in natural conversation by writing a response to this message in the style of your personality. You may use sarcasm or irony, swear, criticize, threat, lie, and engage in personal attacks. You can also bring up a related topic that the post made you think of. [Respond with the number of the message you chose, followed by your comment, for instance: '1. You are completely wrong, John!'. Respond to only 1 message thread. Refer ONLY to messages in this thread. Your message should be {word_length} words long. Do not respond to a message sent by yourself. Do NOT repeat the message you are responding to. Do not use quotation marks. Use informal language. Don't use hashtags.]'''\n",
    "\n",
    "\n",
    "def like_dislike_or_ignore_prompt(timeline, use_dislikes = False):\n",
    "    \n",
    "    # m = '\\n'.join([f'{i}. \"{s}\"' for i, s in enumerate(messages, 1)])    \n",
    "    m = timeline_to_string(timeline)\n",
    "    \n",
    "    return f'''You are using a social media platform, and see the following messages: \n",
    "    \n",
    "{m}\n",
    "\n",
    "Based on your persona, decide if you want to react to each message. Your possible actions are 'press like', {\"'press dislike' \" if use_dislikes else \"\"}and 'no action'. Only like messages that you endorse, and that you feel positive about. [Answer in the format \"1. press like. 2. no action.{\" 3. press dislike \" if use_dislikes else \"\"}\" etc, WITHOUT any linebreaks. Do not motivate your answers.]\n",
    "    '''\n",
    "\n",
    "# Evaluate the resulting timeline, for assessment\n",
    "# def evaluate_timeline_prompt(timeline, MAXIMUM_COMMENTS_TO_SHOW):\n",
    "    \n",
    "#     df = timeline\n",
    "#     result = \"\"\n",
    "#     for index, content, responses, name in zip(range(len(df)),df.content, df.responses, df.from_name):\n",
    "#         result += \"\\n\"\n",
    "#         result += f\"{index+1}. {name}: {content.strip()}\\n\"\n",
    "\n",
    "# #         if len(responses) > MAXIMUM_COMMENTS_TO_SHOW:\n",
    "# #             responses = responses[-MAXIMUM_COMMENTS_TO_SHOW:]\n",
    "\n",
    "# #         for i, reply in enumerate(responses):\n",
    "# #             # print(f\"{index+1}.{i+1}  {reply['content'].strip()}\")\n",
    "# #             result += f\"{index+1}.{i+1} {reply['from_name']}: {reply['content'].strip()}\\n\"\n",
    "    \n",
    "#     #Reaction\n",
    "#     return f'''You are using a social media platform, and see the following timeline: \n",
    "    \n",
    "#     {result}\n",
    "\n",
    "#     Answer the following questions about the timeline:\n",
    "#     1. How conflictual is the timeline? [0-10]\n",
    "#     2. How political is the timeline? [0-10]\n",
    "#     3. To what degree are different perspectives represented? [0-10]\n",
    "#     4. How angry is the timeline? [0-10]\n",
    "#     [Answer in the format 'Conflictual: 6; Political: 8; Perspectives: 5; Angry: 3' Do not motivate your answers.]\n",
    "#     '''\n",
    "\n",
    "\n",
    "def measure_toxicity(message):\n",
    "    # print(message)\n",
    "    nr_tries = 5\n",
    "    tox = None\n",
    "    while(tox is None):\n",
    "        try:\n",
    "\n",
    "            analyze_request = {\n",
    "              'comment': { 'text': message },\n",
    "              'requestedAttributes': {'TOXICITY': {}}\n",
    "            }\n",
    "\n",
    "            response = client.comments().analyze(body=analyze_request).execute()\n",
    "            # print(json.dumps(response, indent=2))\n",
    "            tox = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "            # print(tox)\n",
    "            return tox\n",
    "        except Exception as e:\n",
    "            logging.warning(f'Caught exception on trying to measure toxicity. \"{e}\". Message: \"{message}\". Try {nr_tries}.')\n",
    "            nr_tries-=1\n",
    "            if nr_tries==0:\n",
    "                return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a7ff6-4094-40f8-9ad2-ead4df13be4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2eb5e56-2d4a-490c-bae9-ff2c2a9c8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main classes: \n",
    "#Agent is an individual social media user.\n",
    "#Platform is the social media platform\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, platform, aid, party, partisan, how_often_use_twitter, persona, media, name, attribs):\n",
    "        self.friends = []\n",
    "        self.platform = platform\n",
    "        \n",
    "        self.attribs = attribs\n",
    "        \n",
    "        #The id of the agent in the agent array\n",
    "        self.aid = aid\n",
    "        self.persona = persona\n",
    "        self.media = media\n",
    "        self.name = name\n",
    "        \n",
    "        # self.persona += f\"Your name is {self.name}.\"\n",
    "        \n",
    "        self.party = party\n",
    "        self.partisan = partisan\n",
    "        self.how_often_use_twitter = how_often_use_twitter\n",
    "         \n",
    "    def add_friend(self,friend):\n",
    "        self.friends.append(friend)\n",
    "        \n",
    "    def gpt(self,message):\n",
    "        return gpt(self.persona, message)\n",
    "        \n",
    "    def post_first_message(self):\n",
    "        headlines = get_headlines(self.platform.parameter_number_headlines_first_message, self.media)\n",
    "\n",
    "        headlines_text = ''\n",
    "        for i, head in enumerate(headlines):\n",
    "            headlines_text += f\"{i+1}. {head['media_name']}: '{head['title']}'\\n'{head['body']}'\\n\\n\"\n",
    "        \n",
    "        message = self.gpt(first_message_prompt(headlines_text))\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            i, comment = parse_first_message(message)\n",
    "            if i is not None:\n",
    "                chosen_headline = headlines[i]\n",
    "\n",
    "                self.platform.post_message(comment, chosen_headline['media_name'], chosen_headline['title'], self.aid, self.name)\n",
    "                logging.info(f\"{self.name} posted message: {comment}\")\n",
    "                \n",
    "        except:\n",
    "            logging.info(f\"Error parsing response from LLM: {message}\")\n",
    "\n",
    "\n",
    "    def like_or_dislike(self):\n",
    "        #Get all messages that are visible to the user\n",
    "        timeline = self.platform.get_timeline(self,show_replies=False, hide_liked=True)\n",
    "        \n",
    "        if len(timeline)==0:\n",
    "            logging.warning(\"User timeline is empty! User will not respond to any message.\")\n",
    "            return\n",
    "        \n",
    "        # messages = [m for m in timeline.content]\n",
    "        prompt = like_dislike_or_ignore_prompt(timeline,use_dislikes=self.platform.parameter_use_dislikes)\n",
    "        response = self.gpt(prompt)\n",
    "\n",
    "        try:\n",
    "            actions = parse_action_string(response)\n",
    "        \n",
    "\n",
    "            for index,action in actions.items():\n",
    "                if 'press like' in action:\n",
    "                    self.platform.like_message(timeline.index[index-1],self.aid)\n",
    "                elif 'press dislike' in action:\n",
    "                    self.platform.dislike_message(timeline.index[index-1],self.aid)\n",
    "        except Exception as e:\n",
    "            logging.warning(f'WARNING. Caught exception \"{e}\" when parsing action string: \"{response}\". Keeping on, moving along.')\n",
    "        \n",
    "        \n",
    "    def respond_to_message(self):\n",
    "        \n",
    "        timeline = self.platform.get_timeline(self, show_replies = self.platform.parameter_number_replies_when_commenting!=0,hide_commented=True)\n",
    "        \n",
    "        if len(timeline)==0:\n",
    "            logging.warning(\"User timeline is empty! User will not respond to any message.\")\n",
    "            return\n",
    "        \n",
    "        prompt = respond_to_message_prompt(timeline,self.platform.parameter_number_replies_when_commenting)\n",
    "        response = self.gpt(prompt)\n",
    "\n",
    "        nr, content = parse_respond_to_message(response)\n",
    "        \n",
    "        if nr is not None:\n",
    "            try:\n",
    "                mid = timeline.index[nr]\n",
    "                self.platform.respond_to_message(mid,content,self.aid, self.name, self.party)\n",
    "\n",
    "                logging.info(f\"{self.name} replied to message: {content}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Warning: {e}. GPT responded to non-existing message. DEBUG DATA: \\n ---------- \\n NR: '{nr}'.\\n ------- \\n FULL RESPONSE: '{response}' \\n --------\\n TIMELINE: {timeline_to_string(timeline)} .\")\n",
    "        \n",
    "# Platform\n",
    "class Platform:\n",
    "    # NUMBER_OF_HEADLINES_FIRST_MESSAGE = 10\n",
    "    def __init__(self,parameter_algorithm = 'engagement', parameter_hide_own_messages = False, parameter_number_messages = 10, parameter_fraction_random=0, parameter_use_dislikes = False, parameter_number_replies_when_commenting=0, parameter_comment_weight=1, parameter_number_headlines_first_message=10):\n",
    "        self.messages = pd.DataFrame([], columns=['content','media_source','headline','responses', 'likes', 'dislikes', 'from_aid', 'from_name'])\n",
    "        self.agents = []\n",
    "        \n",
    "        self.set_parameters(parameter_algorithm, parameter_hide_own_messages, parameter_number_messages,parameter_fraction_random,parameter_use_dislikes,parameter_number_replies_when_commenting,parameter_comment_weight,parameter_number_headlines_first_message)\n",
    "        \n",
    "    def set_parameters(self, parameter_algorithm = 'engagement', parameter_hide_own_messages = False, parameter_number_messages = 10, parameter_fraction_random=0,parameter_use_dislikes=False,parameter_number_replies_when_commenting=0,parameter_comment_weight=1,parameter_number_headlines_first_message=10):\n",
    "        self.parameter_algorithm = parameter_algorithm\n",
    "        self.parameter_hide_own_messages = parameter_hide_own_messages\n",
    "        self.parameter_number_messages = parameter_number_messages\n",
    "        self.parameter_fraction_random = parameter_fraction_random\n",
    "        self.parameter_use_dislikes = parameter_use_dislikes\n",
    "        self.parameter_number_replies_when_commenting = parameter_number_replies_when_commenting\n",
    "        self.parameter_comment_weight = parameter_comment_weight\n",
    "        self.parameter_number_headlines_first_message = parameter_number_headlines_first_message\n",
    "\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        param = {}\n",
    "        param['parameter_algorithm'] = self.parameter_algorithm\n",
    "        param['parameter_hide_own_messages'] = self.parameter_hide_own_messages\n",
    "        param['parameter_number_messages'] = self.parameter_number_messages\n",
    "        param['parameter_fraction_random'] = self.parameter_fraction_random\n",
    "        param['parameter_use_dislikes'] = self.parameter_use_dislikes\n",
    "        param['parameter_number_replies_when_commenting'] = self.parameter_number_replies_when_commenting\n",
    "        param['parameter_comment_weight'] = self.parameter_comment_weight\n",
    "        param['parameter_number_headlines_first_message'] = self.parameter_number_headlines_first_message\n",
    "        \n",
    "        return param\n",
    "        \n",
    "    ####### PLATFORM POSTING LOGIC ##########\n",
    "        \n",
    "    def post_message(self,content,media_source, headline, from_aid, from_name):        \n",
    "        self.messages.loc[len(self.messages)] = [content, media_source, headline, [], [], [], from_aid, from_name] \n",
    "        \n",
    "    def respond_to_message(self, mid, content, from_aid, from_name, from_party):\n",
    "        self.messages.loc[mid, 'responses'].append({'content':content,'from_aid':from_aid, 'from_name':from_name, 'from_party':from_party})\n",
    "        \n",
    "    def like_message(self,mid,aid):\n",
    "        if aid not in self.messages.loc[mid, 'likes']:\n",
    "            self.messages.loc[mid, 'likes'].append(aid)\n",
    "    \n",
    "    def dislike_message(self,mid,aid):\n",
    "        if aid not in self.messages.loc[mid, 'dislikes']:\n",
    "            self.messages.loc[mid, 'dislikes'].append(aid)\n",
    "    \n",
    "    #The algorithm returns the timeline for a given user \n",
    "    def get_timeline(self,agent=None,show_replies=True,universal_timeline=False,hide_liked=False,hide_commented=False):\n",
    "        \n",
    "        if agent is None and self.parameter_hide_own_messages:\n",
    "            raise ValueError(\"Exception: cannot use user specific features without providing an agent.\")\n",
    "        \n",
    "        # messages = platform.get_messages()\n",
    "        timeline = self.messages.copy()\n",
    "\n",
    "        friend_aids = set([friend.aid for friend in agent.friends])\n",
    "        \n",
    "        # Precalculations \n",
    "        timeline['from_party'] = [self.agents[a].party for a in timeline['from_aid']]\n",
    "        timeline['same_party'] = [1 if from_party == agent.party else 0 for from_party in timeline['from_party']]\n",
    "        timeline['republican_likes'] = [sum([1 if self.agents[liker_aid].party == 'Republican' else 0 for liker_aid in likes]) for likes in timeline['likes']]\n",
    "        timeline['democrat_likes'] = [sum([1 if self.agents[liker_aid].party == 'Democrat' else 0 for liker_aid in likes]) for likes in timeline['likes']]\n",
    "        timeline['nonpartisan_likes'] = [sum([1 if self.agents[liker_aid].party == 'Non-partisan' else 0 for liker_aid in likes]) for likes in timeline['likes']]\n",
    "        timeline['friend_commented'] = [len(friend_aids.intersection(set([c['from_aid'] for c in comments]))) for comments in timeline.responses]\n",
    "        timeline['friend_liked'] = [len(friend_aids.intersection(set(likes))) for likes in timeline.likes]\n",
    "\n",
    "        if not universal_timeline:\n",
    "            \n",
    "            #Don't show the users' own messages\n",
    "            if self.parameter_hide_own_messages:\n",
    "                timeline = timeline.loc[timeline.from_aid != agent.aid]\n",
    "\n",
    "            #Mark if they are friends or not        \n",
    "            timeline['is_friend'] = [True if from_aid in friend_aids else False for from_aid in timeline['from_aid']]\n",
    "\n",
    "        # The agents see only messages posted by their friends. These are selected and sorted according to the number of likes they receive.\n",
    "        if self.parameter_algorithm == 'posted-by-friends-and-liked':\n",
    "            if not universal_timeline:\n",
    "                timeline = timeline.loc[timeline.is_friend]\n",
    "            timeline['score'] = [len(likes) for comments, likes, dislikes in zip(timeline['responses'],timeline['likes'], timeline['dislikes'])]\n",
    "\n",
    "        elif self.parameter_algorithm == 'posted-by-friends-and-engaged':\n",
    "            if not universal_timeline:\n",
    "                timeline = timeline.loc[timeline.is_friend]\n",
    "            timeline['score'] = [len(likes)+len(comments)*self.parameter_comment_weight+len(dislikes) for comments, likes, dislikes in zip(timeline['responses'],timeline['likes'], timeline['dislikes'])]\n",
    "            \n",
    "        # The agents see all posts â€œlikedâ€ by their friends, selected and sorted by the number of likes they receive. \n",
    "        elif self.parameter_algorithm == 'liked-by-friends':\n",
    "            if not universal_timeline:\n",
    "                timeline = timeline.loc[(timeline['friend_liked']>0)|(timeline.is_friend)]\n",
    "            timeline['score'] = [len(likes) for comments, likes, dislikes in zip(timeline['responses'],timeline['likes'], timeline['dislikes'])]\n",
    "\n",
    "        # The agents see all posts â€œengaged withâ€ by their friends, selected and sorted by the number of engagement they receive. \n",
    "        elif self.parameter_algorithm == 'engaged-by-friends':\n",
    "            if not universal_timeline:\n",
    "                timeline = timeline.loc[(timeline['friend_commented']>0)|(timeline['friend_liked']>0)|(timeline.is_friend)]\n",
    "            timeline['score'] = [len(likes)+len(comments)*self.parameter_comment_weight+len(dislikes) for comments, likes, dislikes in zip(timeline['responses'],timeline['likes'], timeline['dislikes'])]\n",
    "\n",
    "        #The agents see messages from the entire network, selected and sorted according to the number of likes that they receive from friends of the agent.\n",
    "        elif self.parameter_algorithm == 'liked-by-friends-optimization':\n",
    "            timeline['score'] = [len(likes)+len(comments)*self.parameter_comment_weight+len(dislikes) for comments, likes, dislikes in zip(timeline['responses'],timeline['likes'], timeline['dislikes'])]\n",
    "\n",
    "        # The agents see messages from the entire network, selected and sorted according to the number of likes that they receive.    \n",
    "        elif self.parameter_algorithm == 'likes-everyone':\n",
    "            timeline['score'] = [len(likes) for comments, likes, dislikes in zip(timeline['responses'],timeline['likes'], timeline['dislikes'])]\n",
    "\n",
    "        # The agents see messages from the entire network, selected and sorted according to the number of likes that they receive.    \n",
    "        elif self.parameter_algorithm == 'likes-minus-dislikes-everyone':\n",
    "            timeline['score'] = [len(likes)-len(dislikes) for comments, likes, dislikes in zip(timeline['responses'],timeline['likes'], timeline['dislikes'])]\n",
    "\n",
    "        # The agents see messages from the entire network, selected and sorted according to the number of engagements that they receive.    \n",
    "        elif self.parameter_algorithm == 'engagement-everyone':\n",
    "            timeline['score'] = [len(likes)+len(comments)*self.parameter_comment_weight+len(dislikes) for comments, likes, dislikes in zip(timeline['responses'],timeline['likes'], timeline['dislikes'])]\n",
    "\n",
    "        # The agents see messages from the entire network, selected and sorted according to the number of engagements that they receive.    \n",
    "        elif self.parameter_algorithm == 'partisan-bridging':\n",
    "            timeline['score'] = [min(likes_by_democrat,likes_by_republicans)+likes_by_nonpartisan for likes_by_democrat,likes_by_republicans, likes_by_nonpartisan in zip(timeline['democrat_likes'], timeline['republican_likes'], timeline['nonpartisan_likes'])]\n",
    "\n",
    "        elif self.parameter_algorithm == 'partisan-bridging2':\n",
    "            #Only likes from the other side counts: republican posts that are liked by democrats, and vice versa. \n",
    "            timeline['score'] = [likes_by_democrat if from_party == 'Republican' else likes_by_republicans if from_party == 'Democrat' else 0 for likes_by_democrat,likes_by_republicans, likes_by_nonpartisan, from_party in zip(timeline['democrat_likes'], timeline['republican_likes'], timeline['nonpartisan_likes'], timeline['from_party'])]\n",
    "            \n",
    "        # elif \n",
    "            #Posts from the other side that are liked by users from your side are highlighted\n",
    "            \n",
    "        # Random order\n",
    "        elif self.parameter_algorithm == 'random':\n",
    "            timeline['score'] = [0 for likes_by_democrat,likes_by_republicans, likes_by_nonpartisan in zip(timeline['democrat_likes'], timeline['republican_likes'], timeline['nonpartisan_likes'])]\n",
    "\n",
    "        else:\n",
    "            raise Exception(f'Need to specify a valid timeline algorithm: {self.parameter_algorithm}')\n",
    "\n",
    "        # We want to sort the ones with the same \"like value\" in random order. \n",
    "        timeline['random_order'] = np.random.rand(len(timeline))\n",
    "\n",
    "        if len(timeline) == 0:\n",
    "            logging.warning('Get_timeline function: timeline is empty.')\n",
    "            return timeline\n",
    "        \n",
    "        if not universal_timeline:\n",
    "            \n",
    "            if hide_liked:\n",
    "                timeline = timeline[timeline['likes'].apply(lambda x: agent.aid not in x)]        \n",
    "\n",
    "            if hide_commented:\n",
    "                timeline = timeline[timeline['responses'].apply(lambda x: agent.aid not in [a['from_aid'] for a in x])]        \n",
    "            \n",
    "            #Get the top N and a random sample of N_random\n",
    "            if self.parameter_fraction_random>0:\n",
    "                timeline = pd.concat([timeline.sort_values(['score','random_order'],ascending=False).head(round((1-self.parameter_fraction_random)*self.parameter_number_messages)), timeline.sample(n=min(round(self.parameter_fraction_random*self.parameter_number_messages),len(timeline)))])\n",
    "            else:\n",
    "                timeline = timeline.sort_values(['score','random_order'],ascending=False).head(self.parameter_number_messages)\n",
    "\n",
    "        else:\n",
    "            #With universal move, we include all messages\n",
    "            timeline = timeline.sort_values(['score','random_order'],ascending=False)\n",
    "\n",
    "        timeline = timeline.drop(columns=['random_order'])\n",
    "\n",
    "        if not show_replies:\n",
    "            timeline['responses'] = None\n",
    "\n",
    "        return timeline\n",
    "    \n",
    "    \n",
    "    ###### MODEL LOGIC ##########\n",
    "    \n",
    "    def create_agents(self,nragents=20, nr_friends=20, homophily=3.0):\n",
    "        \n",
    "        # anes = get_anes_rows(number_rows=nragents)\n",
    "        anes = get_anes_agents(nragents)\n",
    "\n",
    "        agents = []\n",
    "        for i in range(nragents): \n",
    "            agents.append(Agent(self, aid=i, how_often_use_twitter=anes[i]['howOftenUseTwitter'], party=anes[i]['party'], partisan=anes[i]['partisan'], persona=anes[i]['persona']+\"\\n\"+anes[i]['extended_persona'], media=anes[i]['media'], name = anes[i]['name'] ,attribs=anes[i]['attribs']))\n",
    "        \n",
    "        #Create social network\n",
    "        # G = generate_homophilous_network(partisan, num_connections, homophily_parameter, names)\n",
    "        self.network = generate_homophilous_network(partisan=[a.partisan for a in agents], num_connections = nr_friends, homophily_parameter=homophily, names=[a.name for a in agents])\n",
    "        \n",
    "        #Create network in the agents friends lists \n",
    "        for a,b in self.network.edges:\n",
    "            agents[a].add_friend(agents[b])\n",
    "            agents[b].add_friend(agents[a])\n",
    "        \n",
    "        self.agents = agents\n",
    "    \n",
    "    def plot_social_network(self):\n",
    "        partisan = [a.partisan for a in self.agents]\n",
    "        G = self.network\n",
    "        cmap = mcolors.LinearSegmentedColormap.from_list('RedGreyBlue', [(1, 0, 0), (0.5, 0.5, 0.5), (0, 0, 1)])\n",
    "        plt.figure(figsize=(10, 10), dpi=300)\n",
    "        nx.draw(G, nx.spring_layout(G)  , with_labels=True, node_color=partisan, cmap=cmap)\n",
    "        \n",
    "    def post_first_messages(self, fraction_agents=0.2):\n",
    "        \n",
    "        nr_posts = int(fraction_agents*len(self.agents))\n",
    "                \n",
    "        #Post the messages\n",
    "        #Only a sample of bots actually post: parameter_nr_initial_posts\n",
    "        # poster_agents = random.choices(self.agents, k=platform.parameter_nr_initial_posts, weights=[a.how_often_use_twitter if len(a.media)>0 else 0 for a in self.agents]) \n",
    "        #This only includes users who said they watch any media. But few answerd that question, so it's prohibitively limiting. Now we instead allow those to post, and they news from anywhere.\n",
    "        poster_agents = random.choices(self.agents, k=nr_posts, weights=[a.how_often_use_twitter for a in self.agents]) \n",
    "\n",
    "        progress_bar = tqdm(total=nr_posts, desc=\"Posting first messages\", unit=\"task\")\n",
    "\n",
    "        for agent in poster_agents:\n",
    "            agent.post_first_message()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "    \n",
    "    # def like_and_comment_loop(self,fraction_user_liking,comment_steps):\n",
    "    #This integrated them to one GPT call. I've thought better of it.\n",
    "    # agent.respond_to_message()\n",
    "#     \n",
    "    def user_react_loop(self,comments_per_like=1, fraction_user_likes=0.2):\n",
    "        \n",
    "        number_agents_who_like = int(fraction_user_likes*len(self.agents))\n",
    "        liking_agents = random.choices(self.agents, weights=[a.how_often_use_twitter for a in self.agents], k = number_agents_who_like)\n",
    "\n",
    "        progress_bar = tqdm(total=number_agents_who_like, desc=\"Users liking and replying to messages\", unit=\"task\")\n",
    "        \n",
    "        for liker in liking_agents:\n",
    "            \n",
    "            if random.random() < comments_per_like:                \n",
    "                for agent in random.choices(self.agents, weights=[a.how_often_use_twitter for a in self.agents],k = int(np.ceil(comments_per_like))):\n",
    "                    agent.respond_to_message()\n",
    "            \n",
    "            agent.like_or_dislike()\n",
    "            progress_bar.update(1)\n",
    "        progress_bar.close()\n",
    "        \n",
    "\n",
    "    def user_like_loop(self,fraction_agents=0.2):        \n",
    "        number_agents_who_like = int(fraction_agents*len(self.agents))\n",
    "        \n",
    "        #First, we have the agents like messages. \n",
    "        liking_agents = random.choices(self.agents, weights=[a.how_often_use_twitter for a in self.agents], k = number_agents_who_like)\n",
    "        \n",
    "        progress_bar = tqdm(total=number_agents_who_like, desc=\"Users liking messages\", unit=\"task\")\n",
    "        \n",
    "        for agent in liking_agents:\n",
    "            #pick a random agent\n",
    "            # agent = random.sample(self.agents, 1, weights=[p for p in self.agents.how_often_use_twitter])[0]                        \n",
    "            agent.like_or_dislike()\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "        progress_bar.close()\n",
    "        \n",
    "    def user_comment_loop(self, fraction_agents=0.2):\n",
    "        number_agents_who_comment = int(fraction_agents*len(self.agents))\n",
    "        \n",
    "        progress_bar = tqdm(total=number_agents_who_comment, desc=\"Users commenting on messages\", unit=\"task\")\n",
    "        \n",
    "        for step in range(number_agents_who_comment):\n",
    "            agent = random.choices(self.agents, weights=[a.how_often_use_twitter for a in self.agents])[0]\n",
    "            \n",
    "            #This integrated them to one message. I've thought better of it.\n",
    "            agent.respond_to_message()\n",
    "            \n",
    "            # agent.like_or_dislike()\n",
    "            # agent.react_to_timeline()\n",
    "            progress_bar.update(1)\n",
    "        progress_bar.close()\n",
    "        \n",
    "        \n",
    "#     def run_model(self,fraction_who_likes, steps=10):\n",
    "        \n",
    "#         self.post_first_messages()\n",
    "        \n",
    "#         self.user_like_loop(fraction_who_likes)\n",
    "        \n",
    "#         self.user_comment_loop(steps)\n",
    "        \n",
    "#         # like_and_comment_loop(steps)\n",
    "                            \n",
    "#         print(f\"Run complete.\")\n",
    "     \n",
    "    ###### ANALYZE THE RESULTS ##########\n",
    "    #Analyze the resulting timeline \n",
    "\n",
    "\n",
    "    def get_timeline_with_visibility(self):\n",
    "        m = self.get_timeline(self.agents[0], universal_timeline=True)\n",
    "        m['visibility'] = 0\n",
    "        L = [self.get_timeline(agent) for agent in self.agents]\n",
    "\n",
    "        for index, row in m.iterrows():\n",
    "            # Count the number of dataframes in 'L' that contain the current row\n",
    "            count = sum(row.isin(df.values.flatten()).all() for df in L)\n",
    "            # Update the 'visibility' column with the count\n",
    "            m.at[index, 'visibility'] = count\n",
    "        return m\n",
    "    \n",
    "\n",
    "    def get_timeline_with_toxicity(self):\n",
    "        m = self.get_timeline_with_visibility()\n",
    "        #Measures toxicity of all messages\n",
    "        m['toxicity'] = [measure_toxicity(mess) for mess in m['content']]\n",
    "        m['toxicity_replies'] = [[measure_toxicity(com['content']) for com in responses] for responses in m['responses']]\n",
    "        return m\n",
    "            \n",
    "    def get_timeline_as_text(self):\n",
    "        return timeline_to_string( self.get_timeline(self.agents[0], universal_timeline=True), include_user_information=True )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ff049-208e-409f-b51e-4ba3c17e4165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0915a761-647e-4c89-955a-dc89f08e29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/petter/My Drive/PAPERS/CURRENT/2023 Bail From actors to LLMs/runs/BigRun8/run_posted-by-friends-and-engaged.pkl','rb') as file:\n",
    "#     p = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d832a51-8ad5-4bb2-8bac-7328a4b2b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = p.get_timeline_with_toxicity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65a149d7-3562-4797-86f8-0212726e24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum([tox*visibility*len(c) for tox,visibility,c in zip(m['toxicity'],m['visibility'],m['responses'])  if tox is not None and not np.isnan(tox)]),sum([visibility*len(c) for visibility,c in zip(m['visibility'],m['responses'])]) \n",
    "# # [tox*visibility*len(c) for tox,visibility,c in zip(m['toxicity'],m['visibility'],m['responses'])  if tox is not None and not np.isnan(tox)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4876b9-450d-4880-993c-97985f685a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0187b377-b131-4b84-9ece-83eb2af39402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEASURES TO EVALUTATE THE RESULTING TIMELINE\n",
    "# NOTE: THESE ARE NOT USED!\n",
    "\n",
    "# Possible additional:\n",
    "#  - How many republicans see of democratic posts\n",
    "\n",
    "#Helper function\n",
    "def gpt_evaluate_timeline(prompt, timeline):\n",
    "    \n",
    "    number_of_messages_to_evaluate = 30\n",
    "    \n",
    "    # timeline = platform.get_timeline(agent,universal_timeline=True)\n",
    "    timeline = timeline.sort_values(['score'],ascending=False).head(number_of_messages_to_evaluate)\n",
    "    timeline_text = timeline_to_string(timeline)\n",
    "    \n",
    "    prompt = prompt + \"\\n\\n\" + timeline_text\n",
    "    \n",
    "    response = gpt('You are a bot that evaluates how social media timelines are perceived and experienced by users.',prompt, model='gpt-4', wait_time=1.5)\n",
    "\n",
    "    return int(response)\n",
    "\n",
    "\n",
    "def evaluate_timeline_toxicity(m):\n",
    "    toxicity_report = {}\n",
    "    #Overall mean toxicity\n",
    "    toxicity_report['mean_toxicity_per_reply_impression'] = sum([tox*visibility*len(c) for tox,visibility,c in zip(m['toxicity'],m['visibility'],m['responses'])  if tox is not None and not np.isnan(tox)])/sum([visibility*len(c) for visibility,c in zip(m['visibility'],m['responses'])]) if sum([visibility*len(c) for visibility,c in zip(m['visibility'],m['responses'])]) > 0 else None\n",
    "    toxicity_report['mean_toxicity_per_reply'] = sum([tox*len(c) for tox,c in zip(m['toxicity'],m['responses']) if tox is not None and not np.isnan(tox)])/sum([len(c) for c in m['responses']]) if sum([len(c) for c in m['responses']]) > 0 else None\n",
    "\n",
    "    toxicity_report['mean_toxicity_per_message'] = m['toxicity'].mean()\n",
    "    toxicity_report['mean_toxicity_message_impression'] = sum([tox*visibility/sum(m.visibility) for tox,visibility in zip(m['toxicity'],m['visibility']) if tox is not None and not np.isnan(tox)])\n",
    "\n",
    "    tox = [(t,visibility) for tox_rep,visibility in zip(m['toxicity_replies'],m['visibility']) for t in tox_rep if t is not None and not np.isnan(t)]+[(toxicity,visibility) for toxicity,visibility in zip(m['toxicity'],m['visibility']) if toxicity is not None and not np.isnan(toxicity)]\n",
    "    toxicity_report['toxicity_indicator'] = sum([t*v for t,v in tox])/sum([v for t,v in tox])\n",
    "\n",
    "    # Measure toxicity between D/Rs, vs within D and within R\n",
    "    # dem_to_dem = np.mean([tox for toxicities,responses,from_party in zip(m['toxicity_replies'],m['responses'],m['from_party']) if from_party=='Democrat' for r,tox in zip(responses,toxicities) if r['from_party']=='Democrat'])\n",
    "    # interpartisan = np.mean([tox for toxicities,responses,from_party in zip(m['toxicity_replies'],m['responses'],m['from_party']) if from_party=='Democrat' for r,tox in zip(responses,toxicities) if r['from_party']=='Republican']+[tox for toxicities,responses,from_party in zip(m['toxicity_replies'],m['responses'],m['from_party']) if from_party=='Republican' for r,tox in zip(responses,toxicities) if r['from_party']=='Democrat'])\n",
    "    # rep_to_rep = np.mean([tox for toxicities,responses,from_party in zip(m['toxicity_replies'],m['responses'],m['from_party']) if from_party=='Republican' for r,tox in zip(responses,toxicities) if r['from_party']=='Republican'])\n",
    "\n",
    "    toxicity_report['mean_intrapartisan_toxicity'] = np.nanmean([tox if tox is not None and not np.isnan(tox) else np.nan for toxicities,responses,from_party in zip(m['toxicity_replies'],m['responses'],m['from_party']) if from_party=='Democrat' for r,tox in zip(responses,toxicities) if r['from_party']=='Democrat']+[tox if tox is not None  and not np.isnan(tox) else np.nan for toxicities,responses,from_party in zip(m['toxicity_replies'],m['responses'],m['from_party']) if  from_party=='Republican' for r,tox in zip(responses,toxicities) if r['from_party']=='Republican'])\n",
    "    toxicity_report['mean_interpartisan_toxicity'] = np.nanmean([tox if tox is not None and not np.isnan(tox) else np.nan for toxicities,responses,from_party in zip(m['toxicity_replies'],m['responses'],m['from_party']) if from_party=='Democrat' for r,tox in zip(responses,toxicities) if r['from_party']=='Republican']+[tox if tox is not None and not np.isnan(tox) else np.nan for toxicities,responses,from_party in zip(m['toxicity_replies'],m['responses'],m['from_party']) if from_party=='Republican' for r,tox in zip(responses,toxicities) if r['from_party']=='Democrat'])\n",
    "    # toxicity_report['mean_toxicity_within_democrats'] = sum([tox*len(c) for tox,c in zip(m['toxicity'],m['responses'])])/sum([len(c) for c in m['responses']])\n",
    "\n",
    "    toxicity_report['toxicity_overimpression'] = toxicity_report['mean_toxicity_message_impression']/toxicity_report['mean_toxicity_per_message']\n",
    "    return toxicity_report\n",
    "\n",
    "\n",
    "def evaluate_cross_partisan_likes(platform,timeline):\n",
    "    \n",
    "    mes = Counter(timeline.from_party)\n",
    "    rep = Counter([platform.agents[r].party for likes in timeline.likes for r in likes])\n",
    "\n",
    "    fraction_democratic_messages = mes['Democrat']/(mes['Democrat']+mes['Republican'])\n",
    "    fraction_republican_messages = mes['Republican']/(mes['Democrat']+mes['Republican'])\n",
    "    fraction_democrat_likes = rep['Democrat']/(rep['Democrat']+rep['Republican']) \n",
    "    fraction_republican_likes = rep['Republican']/(rep['Democrat']+rep['Republican'])\n",
    "\n",
    "    ei_expectation_likes = (fraction_republican_messages*fraction_democrat_likes + fraction_democratic_messages*fraction_republican_likes) - (fraction_democratic_messages*fraction_democrat_likes + fraction_republican_messages*fraction_republican_likes)\n",
    "\n",
    "    #E-I index likes\n",
    "    con = [(message_from_party,platform.agents[r].party) for likes,message_from_party in zip(timeline['likes'],timeline['from_party']) for r in likes]\n",
    "    count = [1 if k[0]!=k[1] else -1 for k in con if k[0] != 'Non-partisan' and k[1] != 'Non-partisan']\n",
    "    \n",
    "    if len(count)==0:\n",
    "        return None,None,None\n",
    "    else:    \n",
    "        ei_likes = sum(count)/len(count)\n",
    "\n",
    "        relative_percentage_difference = ((ei_expectation_likes - ei_likes) / ei_expectation_likes) \n",
    "        return relative_percentage_difference,ei_likes,ei_expectation_likes \n",
    "\n",
    "\n",
    "# - Conversational receptiveness (politeness package)  \n",
    "#This was supposed to use politness package, but it was a pain in the ass to implement, so we're using GPT\n",
    "# Promotes thoughtful conversation means encouraging exchanges that exhibit elements of deliberation, \n",
    "# such as when people are receptive to othersâ€™ perspectives or share the reasoning behind their opinions \n",
    "# def evaluate_conversational_receptiveness(timeline):\n",
    "#     prompt = 'To what extent does this timeline express thoughtful conversations that exhibit elements of deliberation, such as when people are to receptive others\\' perspectives or share the reasoning behind their opinions? [Answer with a number 0-100. 0 means that the conversations are not at all thoughtful or deliberative. 100 means that they are highly thoughtful, reasoning, and deliberative. Do not motivate your answer. Do not use quotation marks.]'\n",
    "#     return gpt_evaluate_timeline(prompt,timeline)\n",
    "    \n",
    "def evaluate_engaging(timeline):\n",
    "    prompt = f'''Please read through the following social media timeline. Assess how engaging the content and discussions are, and rate on a scale of 0 to 100 how likely this timeline is to make users want to continue actively using the platform. 0 means the timeline is completely unengaging and users are unlikely to remain active. 100 means the timeline is highly engaging and likely to increase user activity.\n",
    "Only focus on how engaging the timeline is, not whether you agree with the content or find it ethical. Ignore any names or identifiers.\n",
    "After carefully reading the timeline, summarize the overall engagement on a 0 to 100 scale. [Respond with only a number between 0-100, without quotation marks or explanation]\n",
    "\n",
    "Timeline:'''\n",
    "    return gpt_evaluate_timeline(prompt,timeline)\n",
    "    \n",
    "\n",
    "def evaluate_civil(timeline):\n",
    "    prompt = f'''Please read through the following social media timeline and summarize how civil the conversations are between users. Civil conversations are those in which people avoid profanity, insults, and ad-hominem attacks. Rate the civility on a scale of 0 to 100, where 0 means completely uncivil with high levels of profanity, insults, and personal attacks, and 100 means very civil and respectful discourse throughout. Only focus on assessing the tone and hostility level of the content, not the truthfulness or ethics. Ignore any names or identifiers.\n",
    "[Respond with only a single number between 0-100, without quotation marks or explanation]\n",
    "\n",
    "Timeline:\n",
    "'''\n",
    "    return gpt_evaluate_timeline(prompt,timeline)\n",
    "   \n",
    "    \n",
    "def evaluate_diversity_of_viewspoints(timeline):\n",
    "    prompt = f'''Please read through the following social media timeline and assess the extent to which people involved in the conversations engage with a diversity of views or recognize the benefit of engaging with those who express opposing opinions. Score each conversation on a scale of 0 to 100, where 0 means the conversation reflects just one perspective or ideology, and 100 means there is a very wide range of perspectives, opinions, and ideologies reflected in the timeline. Only focus on assessing the tone and hostility level of the content, not the truthfulness or ethics. Ignore any names or identifiers.\n",
    "When evaluating diversity, consider factors like:\n",
    "Presence of competing political, social, cultural viewpoints\n",
    "Representation of voices from different backgrounds and demographics\n",
    "Discussion of a variety of topics, issues, and current events\n",
    "A mix of attitudes, framings, and takes on issues\n",
    "Exchanges of diverging opinions and disagreements\n",
    "[Respond with only a single number between 0-100, without quotation marks or explanation]\n",
    "\n",
    "Timeline:\n",
    "'''\n",
    "    return gpt_evaluate_timeline(prompt,timeline)\n",
    "        \n",
    "\n",
    "def evaluate_healthy_public(timeline):\n",
    "    prompt = f'''Please read through the following social media timeline. Assess the conversations based on the criteria of a healthy public sphere, rating this on a scale of 0 to 100. A healthy public sphere is characterized by productive discussions aimed at solving shared problems, with intellectual humility and respect for opposing views, even during disagreements.\n",
    "When evaluating, look for:\n",
    "-Conversations focused on addressing issues, not attacks\n",
    "-Willingness to understand other perspectives\n",
    "-Appeals to facts, logic and reasoning\n",
    "-Admitting gaps in one's own knowledge\n",
    "-Restraint in assumptions about others' motives\n",
    "-Disagreements handled with politeness and decorum\n",
    "\n",
    "After reviewing the timeline, rate how closely it resembles a healthy public sphere on a scale of 0 to 100. 0 means it completely fails to meet the criteria described. 100 means it very closely aligns with the ideals of a healthy deliberative forum. \n",
    "[Respond with only a single number between 0-100, without quotation marks or explanation]\n",
    "\n",
    "Timeline:'''\n",
    "    \n",
    "    return gpt_evaluate_timeline(prompt,timeline)\n",
    "    \n",
    "def evaluate_productive(timeline):\n",
    "    prompt = f'''Please read through the following social media timeline and summarize how productive the conversations are between users, or how much they find solutions to shared problems through thoughtful discussion and compromise. Score the timeline on a scale of 0 to 100, where 0 means the timeline conversations are not productive at all and 100 means highly productive, enlightening, and likely to lead to new perspectives. Only focus on assessing the tone of the content, not the truthfulness or ethics. Ignore any names or identifiers.\n",
    "When evaluating productive discourse, look for:\n",
    "- Conversations that enhance understanding on issues\n",
    "- Exchanges where people find common ground\n",
    "- Reasoned debates based on facts and logic\n",
    "- Openness to opposing views and changed stances\n",
    "- Light more than heat - minimal toxic conflict/drama\n",
    "[Respond with only a single number between 0-100, without quotation marks or explanation]\n",
    "Timeline:'''\n",
    "    return gpt_evaluate_timeline(prompt,timeline)\n",
    "    \n",
    "    \n",
    "    \n",
    "def evaluate_conversational_toxicity(timeline):\n",
    "    prompt = f'''Please read through the following social media timeline and summarize how toxic and conflictual the discourse is on a scale of 0 to 100, where 0 means no toxicity/conflict at all and 100 means extremely toxic and conflictual. Only focus on assessing the tone and hostility level of the content, not the truthfulness or ethics. Ignore any names or identifiers.\n",
    "When evaluating, consider factors like:\n",
    "- Use of insults, racial/gender/identity slurs, or threats\n",
    "- Attacks against individuals/groups\n",
    "- Presence of misinformation or conspiracy content\n",
    "- Degree of uncivil, inflammatory, or aggressive language\n",
    "- Prevalence of arguments and charged exchanges\n",
    "- Lack of empathy, respect or good faith discussion\n",
    "After carefully reading the timeline, score the overall toxicity and conflict on the provided scale from 0 to 100. [Respond only with a single number. Do not motivate your answer. Do not use quotation marks.]\n",
    "Timeline:'''\n",
    "    return gpt_evaluate_timeline(prompt,timeline)\n",
    "\n",
    "    \n",
    "    \n",
    "# - Politicalness: whether discussions focus on politics or not (LLM-based measure?)\n",
    "def evaluate_politicalness(timeline):\n",
    "    prompt = 'To what extent does this timeline focus on politics, political discussion or political issues? [Answer with a number 0-100. 0 means that politics is not mentioned. 100 means there are only political posts. Do not motivate your answer. Do not use quotation marks.]'\n",
    "    return gpt_evaluate_timeline(prompt,timeline)\n",
    "\n",
    "\n",
    "# - Fairness/Bias: whether the algorithms â€œunfairly\" benefits Republicans/Democrats (just measuring their exposure relative to the number of messages)\n",
    "#How much more visibility do the republican messages get over how many they are?\n",
    "# This returns the difference in percentage between the expected impressions of republican in fraction posts and the actual impressions of republican posts\n",
    "# It thus theoretically goes between -100 and 100 (where -100 means that all posts were democratic, but only republican posts were seen)\n",
    "def evaluate_partisan_bias(m):\n",
    "    # m['nr_likes'] = [len(likes) for likes in m['likes']]\n",
    "    # m['nr_responses'] = [len(responses) for responses in m['responses']]\n",
    "\n",
    "    # messages_by_party = Counter([self.agents[i].party for i in m.from_aid])\n",
    "    messages_by_party = Counter(list(m.from_party))\n",
    "    messages_impressions_by_party = defaultdict(int,m.groupby(['from_party'])['visibility'].sum().to_dict())\n",
    "\n",
    "    # Fraction republican\n",
    "    expected_republican = messages_by_party['Republican']/(messages_by_party['Republican']+messages_by_party['Democrat'])\n",
    "    actual_republican = messages_impressions_by_party['Republican']/(messages_impressions_by_party['Republican']+messages_impressions_by_party['Democrat'])\n",
    "\n",
    "    return (expected_republican-actual_republican)*100,expected_republican,actual_republican\n",
    "\n",
    "\n",
    "# - Echo chamberness: whether there are discussions between left and right (just measuring interaction) \n",
    "#We calculate the expected E-I value, and the actual E-I value for comments between partisans. \n",
    "def evaluate_echo_chamberness_comments(timeline):\n",
    "\n",
    "    #Calculate the expected E-I index if they were randomly distributed, given the number of messages and replies.\n",
    "    mes = Counter(timeline.from_party)\n",
    "    rep = Counter([r['from_party'] for responses in timeline.responses for r in responses])\n",
    "\n",
    "    fraction_democratic_messages = mes['Democrat']/(mes['Democrat']+mes['Republican'])\n",
    "    fraction_republican_messages = mes['Republican']/(mes['Democrat']+mes['Republican'])\n",
    "    fraction_democrat_replies = rep['Democrat']/(rep['Democrat']+rep['Republican']) \n",
    "    fraction_republican_replies = rep['Republican']/(rep['Democrat']+rep['Republican'])\n",
    "\n",
    "    ei_expectation_replies = (fraction_republican_messages*fraction_democrat_replies + fraction_democratic_messages*fraction_republican_replies) - (fraction_democratic_messages*fraction_democrat_replies + fraction_republican_messages*fraction_republican_replies)\n",
    "\n",
    "    #Calculate actual E-I index comments\n",
    "    #What fraction of posts from republicans are to democrats, vs to other republicans\n",
    "    con = [(message_from_party,r['from_party']) for replies,message_from_party in zip(timeline['responses'],timeline['from_party']) for r in replies]\n",
    "    count = [1 if k[0]!=k[1] else -1 for k in con if k[0] != 'Non-partisan' and k[1] != 'Non-partisan' ]\n",
    "    ei_comments = sum(count)/len(count)\n",
    "    \n",
    "    relative_percentage_difference = 100*((ei_expectation_replies - ei_comments) / ei_expectation_replies) #relative_percentage_difference = (abs(ei_expectation_replies - ei_comments) / ei_expectation_replies) * 100\n",
    "    return relative_percentage_difference,ei_comments,ei_expectation_replies \n",
    "    \n",
    "\n",
    "\n",
    "def evaluate_timeline(platform):\n",
    "    m = platform.get_timeline_with_toxicity()\n",
    "    data = {}\n",
    "    \n",
    "    # Evaluate conversatoinal receptiveness\n",
    "    # data['conversational_receptiveness'] = evaluate_conversational_receptiveness(m)\n",
    "    \n",
    "    # How political is timeline \n",
    "\n",
    "    # Toxicity information\n",
    "    toxicity_report = evaluate_timeline_toxicity(m)\n",
    "    data.update(toxicity_report)\n",
    "    \n",
    "    data['politicalness'] = evaluate_politicalness(m)\n",
    "    data['conversational_toxicity'] = evaluate_conversational_toxicity(m)\n",
    "    data['conversational_productivity'] = evaluate_productive(m)\n",
    "    data['healthy_public'] = evaluate_healthy_public(m)\n",
    "    data['diversity_of_viewspoints'] = evaluate_diversity_of_viewspoints(m)\n",
    "    data['conversational_civility'] = evaluate_civil(m)    \n",
    "    data['conversational_engaging'] = evaluate_engaging(m)        \n",
    "    \n",
    "    # Partisan bias\n",
    "    # data['partisan_bias_rpd'],data['expected_republican'],data['actual_republican'] = evaluate_partisan_bias(m)\n",
    "    _,data['expected_republican'],data['actual_republican'] = evaluate_partisan_bias(m)    \n",
    "\n",
    "    # Echo chamber in comments\n",
    "    relative_percentage_difference,ei_comments,ei_expectation_replies = evaluate_echo_chamberness_comments(m)\n",
    "    # data['echochamber_rpd'] = relative_percentage_difference\n",
    "    data['echochamber_ei_index'] = ei_comments\n",
    "    # data['echochamber_ei_expected'] = ei_expectation_replies\n",
    "    \n",
    "    # Number cross partisan likes  \n",
    "    relative_percentage_difference,ei_likes,ei_expectation_likes  = evaluate_cross_partisan_likes(platform, m)\n",
    "    # data['crosspartisanlikes_rpd'] = relative_percentage_difference\n",
    "    data['crosspartisanlikes_ei_index'] = ei_likes\n",
    "    # data['crosspartisanlikes_ei_expected'] = ei_expectation_likes\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "\n",
    "#     #By news source\n",
    "#     statistics['visibility_by_source'] = m.groupby(['media_source'])['visibility'].sum().to_dict()\n",
    "#     statistics['likes_by_source'] = m.groupby(['media_source'])['nr_likes'].sum().to_dict()\n",
    "#     statistics['replies_by_source'] = m.groupby(['media_source'])['nr_responses'].sum().to_dict()\n",
    "\n",
    "#     return statistics\n",
    "\n",
    "\n",
    "\n",
    "# Get a timeline from a specific user, and use GPT to evaluate how it's experienced\n",
    "# def evaluate_timeline(platform, agent):\n",
    "#     timeline = platform.get_timeline(agent,universal_timeline=True)\n",
    "#     prompt = evaluate_timeline_prompt(timeline)\n",
    "\n",
    "#     response = gpt('You are a bot that evaluates how social media timelines are perceived and experienced by users.',prompt) #Note: this uses the gpt function, not of an agent\n",
    "#     return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcaddeca-956f-4198-815b-393589f76fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "p=None\n",
    "def parameter_run(parameters_algorithm, iterations=1, name='run', parameter_number_messages=10, parameter_hide_own_messages = False, parameter_fraction_random=0.3, parameter_use_dislikes=False, parameter_comment_weight=1,\n",
    "                  nragents=500,nr_friends=30,homophily=5.0, fraction_agents_first_message=0.3, comments_per_like=1, fraction_user_likes=0.3, parameter_number_replies_when_commenting=0, parameter_number_headlines_first_message=10):\n",
    "    \n",
    "    # Creates a new folder for this entire run\n",
    "    # current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    # directory = f'runs/{name}_{current_datetime}'\n",
    "    directory = f'runs/{name}'\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "    \n",
    "    outcomes = []\n",
    "    i = 0\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "    \n",
    "        # We store a dataframe that keeps track of the individual models\n",
    "        for parameter_algorithm in parameters_algorithm:\n",
    "\n",
    "            print(f\"Running model {i}... {parameter_algorithm}\")\n",
    "            filename = f'{directory}/run_{parameter_algorithm}_{iteration}.pkl'\n",
    "\n",
    "            if os.path.exists(filename):\n",
    "                print(\"Run already exists. Skipping!\")\n",
    "                continue\n",
    "\n",
    "            outcome = {}\n",
    "            outcome['filename'] = filename\n",
    "            outcome['rundate'] = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            outcome['index']=i\n",
    "            outcome['iteration']=iteration\n",
    "\n",
    "            p = Platform(parameter_algorithm = parameter_algorithm, parameter_number_messages=parameter_number_messages, \n",
    "                         parameter_hide_own_messages = parameter_hide_own_messages, parameter_fraction_random=parameter_fraction_random, \n",
    "                         parameter_use_dislikes=parameter_use_dislikes, parameter_comment_weight=parameter_comment_weight, parameter_number_replies_when_commenting=parameter_number_replies_when_commenting, parameter_number_headlines_first_message=parameter_number_headlines_first_message)\n",
    "\n",
    "            outcome.update(p.get_parameters())\n",
    "\n",
    "            p.create_agents(nragents=nragents,nr_friends=nr_friends,homophily=homophily)\n",
    "\n",
    "            p.post_first_messages(fraction_agents=fraction_agents_first_message)\n",
    "            p.user_react_loop(comments_per_like=comments_per_like, fraction_user_likes=fraction_user_likes)\n",
    "\n",
    "            with open(filename, 'wb') as file:\n",
    "                pickle.dump(p, file)\n",
    "\n",
    "            outcomes.append(outcome)\n",
    "\n",
    "            pd.DataFrame(outcomes).to_csv(f'{directory}/runlist.csv')\n",
    "\n",
    "            i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d555d7e1-cb6e-4cdc-8f1a-b9574208f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithms = ['posted-by-friends-and-engaged']\n",
    "# parameter_run(algorithms,name='smalltest2', parameter_comment_weight=1, parameter_fraction_random=0.3,nragents=100, parameter_number_messages=20, nr_friends=20, fraction_agents_first_message=0.5, comments_per_like=1, fraction_user_likes=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f441fd35-a6c5-40f4-8625-5911d3060eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.plot_social_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ef5946b3-fd24-43aa-9711-75e3537d8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['posted-by-friends-and-engaged','engagement-everyone','partisan-bridging2'] \n",
    "parameter_run(algorithms,iterations=10,name='BigRun8', parameter_comment_weight=5, parameter_fraction_random=0.0, parameter_number_messages=10, nr_friends=30, \n",
    "              fraction_agents_first_message=0.4, comments_per_like=1, fraction_user_likes=0.3, parameter_number_replies_when_commenting=0,parameter_number_headlines_first_message=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "657590b7-f01e-4571-b118-c2953eaa6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "def evaluate_models(name):\n",
    "    directory = f'runs/{name}'\n",
    "    # runs = pd.read_csv(f'{directory}/runlist.csv').to_dict(orient='records')\n",
    "    \n",
    "    files = glob.glob(os.path.join(directory, '*.pkl'))\n",
    "    \n",
    "    # print(runs)\n",
    "    outcomes = []\n",
    "    # for run in runs:\n",
    "    for file in files:\n",
    "        \n",
    "        # if not os.path.exists(run['filename']):\n",
    "        #     print(f\"Model run file does not exist: {run['filename']}. Skipping...\")\n",
    "        #     continue\n",
    "        \n",
    "        with open(file,'rb') as file:\n",
    "            p = pickle.load(file)\n",
    "    \n",
    "        outcome = {}        \n",
    "        outcome['filename'] = file.name\n",
    "        outcome.update(p.get_parameters())\n",
    "        \n",
    "        print(file.name)\n",
    "        success = False\n",
    "        while(not success):\n",
    "            try:\n",
    "                outcome.update(evaluate_timeline(p))\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Exception in evaluation. {e}. Trying again\")\n",
    "        \n",
    "        # with open(f\"{directory}/timeline_{run['algorithm']}.txt\", 'w') as file:            \n",
    "        with open(f\"{file.name}_timeline.txt\", 'w') as file:\n",
    "            file.write(p.get_timeline_as_text())\n",
    "            \n",
    "        outcomes.append(outcome)    \n",
    "        pd.DataFrame(outcomes).to_csv(f'{directory}/runlist_evaluated.csv')\n",
    "        \n",
    "\n",
    "# evaluate_models('BigRun8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b0ba2-e49f-4593-ac2a-ad8d8932a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8d50a3e2-5a33-48ce-a3f2-915da2da9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.plot_social_network()\n",
    "\n",
    "#Are there any examples of republicans having democratic friends?\n",
    "# with open('runs/BigRun4/run_posted-by-friends-and-engaged.pkl','rb') as file:\n",
    "#     p = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d41e04-af6c-4f3c-b5de-e97ae260029a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "45575c4c-9841-49fc-a6cf-1ddbcf5860ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught exception on trying to measure toxicity. \"[Errno 60] Operation timed out\". Message: \"OMG, can we just fast forward to November already? June was an absolute disaster for our country thanks to Trump. The pandemic is spiraling out of control and he's more focused on photo ops with Bibles than actually leading. We need a leader who will take this seriously and prioritize the health and well-being of Americans. #VoteBlue #DumpTrump\". Try 5.\n",
      "Caught exception on trying to measure toxicity. \"<HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyAxTjb4F0tKxk-X6_s3Nd5E1VHKbok8KuU&alt=json returned \"Attribute TOXICITY does not support request languages: gd\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['gd'], 'attribute': 'TOXICITY'}}]\">\". Message: \"Stop fearmongering, Sarah Thompson!\". Try 5.\n",
      "Caught exception on trying to measure toxicity. \"<HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyAxTjb4F0tKxk-X6_s3Nd5E1VHKbok8KuU&alt=json returned \"Attribute TOXICITY does not support request languages: gd\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['gd'], 'attribute': 'TOXICITY'}}]\">\". Message: \"Stop fearmongering, Sarah Thompson!\". Try 4.\n",
      "Caught exception on trying to measure toxicity. \"<HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyAxTjb4F0tKxk-X6_s3Nd5E1VHKbok8KuU&alt=json returned \"Attribute TOXICITY does not support request languages: gd\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['gd'], 'attribute': 'TOXICITY'}}]\">\". Message: \"Stop fearmongering, Sarah Thompson!\". Try 3.\n",
      "Caught exception on trying to measure toxicity. \"<HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyAxTjb4F0tKxk-X6_s3Nd5E1VHKbok8KuU&alt=json returned \"Attribute TOXICITY does not support request languages: gd\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['gd'], 'attribute': 'TOXICITY'}}]\">\". Message: \"Stop fearmongering, Sarah Thompson!\". Try 2.\n",
      "Caught exception on trying to measure toxicity. \"<HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyAxTjb4F0tKxk-X6_s3Nd5E1VHKbok8KuU&alt=json returned \"Attribute TOXICITY does not support request languages: gd\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['gd'], 'attribute': 'TOXICITY'}}]\">\". Message: \"Stop fearmongering, Sarah Thompson!\". Try 1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09478452425460154"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: toxic indicator: the mean toxicity over impressions of messages and replies\n",
    "# (mean(reply_toxicity)+message_toxicity)*visibility/sum(visibility)\n",
    "m = p.get_timeline_with_toxicity()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38519f0f-d88e-4f86-af72-b6f76ac8efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate_multirun.py\n",
    "\n",
    "import glob \n",
    "\n",
    "def evaluate_models(name):\n",
    "    directory = f'multirun/runs/{name}'\n",
    "    # runs = pd.read_csv(f'{directory}/runlist.csv').to_dict(orient='records')\n",
    "    \n",
    "    files = glob.glob(os.path.join(directory, '*.pkl'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(runs)\n",
    "    outcomes = []\n",
    "    # for run in runs:\n",
    "    for file in files:\n",
    "        \n",
    "        # if not os.path.exists(run['filename']):\n",
    "        #     print(f\"Model run file does not exist: {run['filename']}. Skipping...\")\n",
    "        #     continue\n",
    "        \n",
    "        with open(file,'rb') as file:\n",
    "            p = pickle.load(file)\n",
    "    \n",
    "        outcome = {}        \n",
    "        outcome['filename'] = file.name\n",
    "        outcome.update(p.get_parameters())\n",
    "        \n",
    "        print(file.name)\n",
    "        success = False\n",
    "        while(not success):\n",
    "            try:\n",
    "                outcome.update(evaluate_timeline(p))\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Exception in evaluation. {e}. Trying again\")\n",
    "        \n",
    "        # with open(f\"{directory}/timeline_{run['algorithm']}.txt\", 'w') as file:            \n",
    "        with open(f\"{file.name}_timeline.txt\", 'w') as file:\n",
    "            file.write(p.get_timeline_as_text())\n",
    "            \n",
    "        outcomes.append(outcome)    \n",
    "        pd.DataFrame(outcomes).to_csv(f'{directory}/runlist_evaluated.csv')\n",
    "        \n",
    "\n",
    "evaluate_models('BigRun')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e364e9f0-ed73-41d2-8aa0-9a9340e5e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'multirun/multirun/runs/BigRun'\n",
    "finished = pd.read_csv(f'{directory}/runlist_evaluated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "290add18-8e4b-40da-9941-ba932b166102",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = finished.loc[finished['evaluation_failed']==True]\n",
    "right = finished.loc[finished['evaluation_failed']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35afb7c-9f4a-4790-a427-b5cba3101d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left.filename.values\n",
    "# pd.concat([left,right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0573d478-bb15-42c9-a972-221010df6032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>parameter_hide_own_messages</th>\n",
       "      <th>parameter_number_messages</th>\n",
       "      <th>parameter_fraction_random</th>\n",
       "      <th>parameter_use_dislikes</th>\n",
       "      <th>parameter_number_replies_when_commenting</th>\n",
       "      <th>parameter_comment_weight</th>\n",
       "      <th>parameter_number_headlines_first_message</th>\n",
       "      <th>mean_toxicity_per_reply_impression</th>\n",
       "      <th>mean_toxicity_per_reply</th>\n",
       "      <th>mean_toxicity_per_message</th>\n",
       "      <th>mean_toxicity_message_impression</th>\n",
       "      <th>toxicity_indicator</th>\n",
       "      <th>mean_intrapartisan_toxicity</th>\n",
       "      <th>mean_interpartisan_toxicity</th>\n",
       "      <th>toxicity_overimpression</th>\n",
       "      <th>politicalness</th>\n",
       "      <th>conversational_toxicity</th>\n",
       "      <th>conversational_productivity</th>\n",
       "      <th>healthy_public</th>\n",
       "      <th>diversity_of_viewspoints</th>\n",
       "      <th>conversational_civility</th>\n",
       "      <th>conversational_engaging</th>\n",
       "      <th>expected_republican</th>\n",
       "      <th>actual_republican</th>\n",
       "      <th>echochamber_ei_index</th>\n",
       "      <th>crosspartisanlikes_ei_index</th>\n",
       "      <th>evaluation_failed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parameter_algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>engagement-everyone</th>\n",
       "      <td>27.8125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.066988</td>\n",
       "      <td>0.065244</td>\n",
       "      <td>0.064852</td>\n",
       "      <td>0.061049</td>\n",
       "      <td>0.120878</td>\n",
       "      <td>0.148976</td>\n",
       "      <td>0.163318</td>\n",
       "      <td>0.938760</td>\n",
       "      <td>63.812500</td>\n",
       "      <td>23.812500</td>\n",
       "      <td>58.687500</td>\n",
       "      <td>67.50</td>\n",
       "      <td>50.312500</td>\n",
       "      <td>83.125000</td>\n",
       "      <td>75.562500</td>\n",
       "      <td>0.129254</td>\n",
       "      <td>0.076344</td>\n",
       "      <td>-0.656595</td>\n",
       "      <td>-0.764024</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partisan-bridging2</th>\n",
       "      <td>25.0625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.078756</td>\n",
       "      <td>0.075092</td>\n",
       "      <td>0.065840</td>\n",
       "      <td>0.064015</td>\n",
       "      <td>0.116186</td>\n",
       "      <td>0.143949</td>\n",
       "      <td>0.129137</td>\n",
       "      <td>0.981542</td>\n",
       "      <td>55.375000</td>\n",
       "      <td>26.687500</td>\n",
       "      <td>41.250000</td>\n",
       "      <td>61.75</td>\n",
       "      <td>46.625000</td>\n",
       "      <td>79.187500</td>\n",
       "      <td>70.312500</td>\n",
       "      <td>0.151535</td>\n",
       "      <td>0.559899</td>\n",
       "      <td>0.034390</td>\n",
       "      <td>-0.203158</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posted-by-friends-and-engaged</th>\n",
       "      <td>25.5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.072418</td>\n",
       "      <td>0.075158</td>\n",
       "      <td>0.065463</td>\n",
       "      <td>0.063901</td>\n",
       "      <td>0.101195</td>\n",
       "      <td>0.139788</td>\n",
       "      <td>0.196411</td>\n",
       "      <td>0.978276</td>\n",
       "      <td>77.555556</td>\n",
       "      <td>25.166667</td>\n",
       "      <td>52.722222</td>\n",
       "      <td>68.00</td>\n",
       "      <td>52.166667</td>\n",
       "      <td>82.444444</td>\n",
       "      <td>76.611111</td>\n",
       "      <td>0.144848</td>\n",
       "      <td>0.144044</td>\n",
       "      <td>-0.946227</td>\n",
       "      <td>-0.969228</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unnamed: 0  parameter_hide_own_messages  \\\n",
       "parameter_algorithm                                                      \n",
       "engagement-everyone               27.8125                          0.0   \n",
       "partisan-bridging2                25.0625                          0.0   \n",
       "posted-by-friends-and-engaged     25.5000                          0.0   \n",
       "\n",
       "                               parameter_number_messages  \\\n",
       "parameter_algorithm                                        \n",
       "engagement-everyone                                 10.0   \n",
       "partisan-bridging2                                  10.0   \n",
       "posted-by-friends-and-engaged                       10.0   \n",
       "\n",
       "                               parameter_fraction_random  \\\n",
       "parameter_algorithm                                        \n",
       "engagement-everyone                                  0.0   \n",
       "partisan-bridging2                                   0.0   \n",
       "posted-by-friends-and-engaged                        0.0   \n",
       "\n",
       "                               parameter_use_dislikes  \\\n",
       "parameter_algorithm                                     \n",
       "engagement-everyone                               0.0   \n",
       "partisan-bridging2                                0.0   \n",
       "posted-by-friends-and-engaged                     0.0   \n",
       "\n",
       "                               parameter_number_replies_when_commenting  \\\n",
       "parameter_algorithm                                                       \n",
       "engagement-everyone                                                 0.0   \n",
       "partisan-bridging2                                                  0.0   \n",
       "posted-by-friends-and-engaged                                       0.0   \n",
       "\n",
       "                               parameter_comment_weight  \\\n",
       "parameter_algorithm                                       \n",
       "engagement-everyone                                 5.0   \n",
       "partisan-bridging2                                  5.0   \n",
       "posted-by-friends-and-engaged                       5.0   \n",
       "\n",
       "                               parameter_number_headlines_first_message  \\\n",
       "parameter_algorithm                                                       \n",
       "engagement-everyone                                                15.0   \n",
       "partisan-bridging2                                                 15.0   \n",
       "posted-by-friends-and-engaged                                      15.0   \n",
       "\n",
       "                               mean_toxicity_per_reply_impression  \\\n",
       "parameter_algorithm                                                 \n",
       "engagement-everyone                                      0.066988   \n",
       "partisan-bridging2                                       0.078756   \n",
       "posted-by-friends-and-engaged                            0.072418   \n",
       "\n",
       "                               mean_toxicity_per_reply  \\\n",
       "parameter_algorithm                                      \n",
       "engagement-everyone                           0.065244   \n",
       "partisan-bridging2                            0.075092   \n",
       "posted-by-friends-and-engaged                 0.075158   \n",
       "\n",
       "                               mean_toxicity_per_message  \\\n",
       "parameter_algorithm                                        \n",
       "engagement-everyone                             0.064852   \n",
       "partisan-bridging2                              0.065840   \n",
       "posted-by-friends-and-engaged                   0.065463   \n",
       "\n",
       "                               mean_toxicity_message_impression  \\\n",
       "parameter_algorithm                                               \n",
       "engagement-everyone                                    0.061049   \n",
       "partisan-bridging2                                     0.064015   \n",
       "posted-by-friends-and-engaged                          0.063901   \n",
       "\n",
       "                               toxicity_indicator  \\\n",
       "parameter_algorithm                                 \n",
       "engagement-everyone                      0.120878   \n",
       "partisan-bridging2                       0.116186   \n",
       "posted-by-friends-and-engaged            0.101195   \n",
       "\n",
       "                               mean_intrapartisan_toxicity  \\\n",
       "parameter_algorithm                                          \n",
       "engagement-everyone                               0.148976   \n",
       "partisan-bridging2                                0.143949   \n",
       "posted-by-friends-and-engaged                     0.139788   \n",
       "\n",
       "                               mean_interpartisan_toxicity  \\\n",
       "parameter_algorithm                                          \n",
       "engagement-everyone                               0.163318   \n",
       "partisan-bridging2                                0.129137   \n",
       "posted-by-friends-and-engaged                     0.196411   \n",
       "\n",
       "                               toxicity_overimpression  politicalness  \\\n",
       "parameter_algorithm                                                     \n",
       "engagement-everyone                           0.938760      63.812500   \n",
       "partisan-bridging2                            0.981542      55.375000   \n",
       "posted-by-friends-and-engaged                 0.978276      77.555556   \n",
       "\n",
       "                               conversational_toxicity  \\\n",
       "parameter_algorithm                                      \n",
       "engagement-everyone                          23.812500   \n",
       "partisan-bridging2                           26.687500   \n",
       "posted-by-friends-and-engaged                25.166667   \n",
       "\n",
       "                               conversational_productivity  healthy_public  \\\n",
       "parameter_algorithm                                                          \n",
       "engagement-everyone                              58.687500           67.50   \n",
       "partisan-bridging2                               41.250000           61.75   \n",
       "posted-by-friends-and-engaged                    52.722222           68.00   \n",
       "\n",
       "                               diversity_of_viewspoints  \\\n",
       "parameter_algorithm                                       \n",
       "engagement-everyone                           50.312500   \n",
       "partisan-bridging2                            46.625000   \n",
       "posted-by-friends-and-engaged                 52.166667   \n",
       "\n",
       "                               conversational_civility  \\\n",
       "parameter_algorithm                                      \n",
       "engagement-everyone                          83.125000   \n",
       "partisan-bridging2                           79.187500   \n",
       "posted-by-friends-and-engaged                82.444444   \n",
       "\n",
       "                               conversational_engaging  expected_republican  \\\n",
       "parameter_algorithm                                                           \n",
       "engagement-everyone                          75.562500             0.129254   \n",
       "partisan-bridging2                           70.312500             0.151535   \n",
       "posted-by-friends-and-engaged                76.611111             0.144848   \n",
       "\n",
       "                               actual_republican  echochamber_ei_index  \\\n",
       "parameter_algorithm                                                      \n",
       "engagement-everyone                     0.076344             -0.656595   \n",
       "partisan-bridging2                      0.559899              0.034390   \n",
       "posted-by-friends-and-engaged           0.144044             -0.946227   \n",
       "\n",
       "                               crosspartisanlikes_ei_index  evaluation_failed  \n",
       "parameter_algorithm                                                            \n",
       "engagement-everyone                              -0.764024                0.0  \n",
       "partisan-bridging2                               -0.203158                0.0  \n",
       "posted-by-friends-and-engaged                    -0.969228                0.0  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kuk = pd.read_csv(f'multirun/multirun/runlist_evaluated.csv')\n",
    "kuk = kuk.loc[kuk['evaluation_failed']==False]\n",
    "kuk.groupby(['parameter_algorithm']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a078136d-8989-4a84-9b5f-35e303cecc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob \n",
    "\n",
    "def evaluate_models(name):\n",
    "    directory = f'multirun/runs/{name}'\n",
    "    # runs = pd.read_csv(f'{directory}/runlist.csv').to_dict(orient='records')\n",
    "    \n",
    "    files = glob.glob(os.path.join(directory, '*.pkl'))\n",
    "    \n",
    "    finished = pd.read_csv(f'{directory}/runlist_evaluated.csv')\n",
    "    left = finished.loc[finished['evaluation_failed']==True]\n",
    "    \n",
    "    # print(runs)\n",
    "    outcomes = []\n",
    "    # for run in runs:\n",
    "    for file in left.filename.values:\n",
    "        \n",
    "        # if not os.path.exists(run['filename']):\n",
    "        #     print(f\"Model run file does not exist: {run['filename']}. Skipping...\")\n",
    "        #     continue\n",
    "        \n",
    "        with open(file,'rb') as file:\n",
    "            p = pickle.load(file)\n",
    "    \n",
    "        outcome = {}        \n",
    "        outcome['filename'] = file.name\n",
    "        outcome.update(p.get_parameters())\n",
    "        \n",
    "        print(file.name)\n",
    "        success = False\n",
    "        while(not success):\n",
    "            try:\n",
    "                outcome.update(evaluate_timeline(p))\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Exception in evaluation. {e}. Trying again\")\n",
    "        \n",
    "        # with open(f\"{directory}/timeline_{run['algorithm']}.txt\", 'w') as file:            \n",
    "        with open(f\"{file.name}_timeline.txt\", 'w') as file:\n",
    "            file.write(p.get_timeline_as_text())\n",
    "            \n",
    "        outcomes.append(outcome)\n",
    "        \n",
    "        #Combine new and old and save\n",
    "        pd.concat(finished.loc[finished['evaluation_failed']==False],pd.DataFrame(outcomes)).to_csv(f'{directory}/runlist_evaluated.csv')\n",
    "        \n",
    "evaluate_models('BigRun')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4dd6a-f810-431e-8547-58d90e0b24fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b6fb40c6-e272-4ae4-b596-a39557d003fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(list(filter(lambda x: x is not None, tox_rep)))\n",
    "\n",
    "# m.toxicity_replies\n",
    "# np.nanmean([13,4,5,6,6,23,np.nan])\n",
    "#m['toxicity_replies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "454d5631-9640-4d59-82c0-895907a59bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sum([tox*visibility*len(c) for tox,visibility,c in zip(m['toxicity'],m['visibility'],m['responses'])  if tox is not None])/sum([visibility*len(c) for visibility,c in zip(m['visibility'],m['responses'])]) if sum([visibility*len(c) for visibility,c in zip(m['visibility'],m['responses'])]) > 0 else None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ea02f-001e-4e60-89e8-bec05de43bbc",
   "metadata": {},
   "source": [
    "# Fetch and store news headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca4c67-75d9-45e2-a248-9f1fd4c6c7d7",
   "metadata": {},
   "source": [
    "This codes fetches and downloads news headlines from newsapi.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "06f0a3f9-7402-407a-8bf5-3574b3b7c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventregistry import *\n",
    "import json, os, sys\n",
    "\n",
    "import os\n",
    "\n",
    "def fetch_news_for_date(date):\n",
    "\n",
    "# date = \"2021-10-21\"\n",
    "# date = \"2021-10-21\"\n",
    "\n",
    "    filename = f\"newsarticles/newsarticles{date}.pkl\"\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"The file '{filename}' already exists. Moving on.\")\n",
    "    else:\n",
    "\n",
    "\n",
    "        er = EventRegistry(apiKey = 'a06c9168-65e9-46ea-9827-c8dae754b72f')\n",
    "\n",
    "        urls = [('V201634c','huffpost.com'),\n",
    "            ('V201634a','yahoo.com'), \n",
    "        ('V201634b','edition.cnn.com'),\n",
    "        ('V201634d','nytimes.com'),\n",
    "        ('V201634e','breitbart.com'),\n",
    "        ('V201634f','foxnews.com'),\n",
    "        ('V201634g','washingtonpost.com'),\n",
    "        ('V201634h','theguardian.com'),\n",
    "        ('V201634i','usatoday.com'),\n",
    "        ('V201634j','bbc.com'),\n",
    "        ('V201634k','npr.org'),\n",
    "        ('V201634m','dailycaller.com'),\n",
    "        ('V201634n','bloomberg.com'),\n",
    "        ('V201634p','buzzfeednews.com'),\n",
    "        ('V201634q','nbcnews.com')\n",
    "        ]\n",
    "\n",
    "        l= []\n",
    "        print(len(l))\n",
    "\n",
    "\n",
    "        for k,url in urls:\n",
    "\n",
    "            qStr = \"\"\"\n",
    "            {\n",
    "                \"$query\": {\n",
    "                    \"$and\": [\n",
    "                        {\n",
    "                            \"sourceUri\":\\\"\"\"\" + str(url) + \"\"\"\\\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"dateStart\": \\\"\"\"\"+date+\"\"\"\\\",\n",
    "                            \"dateEnd\": \\\"\"\"\"+date+\"\"\"\\\",\n",
    "                            \"lang\": \"eng\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\"\"\"\n",
    "\n",
    "            q = QueryArticlesIter.initWithComplexQuery(qStr)\n",
    "\n",
    "            for article in q.execQuery(er, maxItems=1000):\n",
    "                article['key'] = k\n",
    "                article['source_url'] = url \n",
    "                l.append(article)\n",
    "                if len(l) % 100 == 0:\n",
    "                    print(len(l))\n",
    "\n",
    "        print(len(l))\n",
    "        ### \n",
    "        # V201634a yahoo.com\n",
    "        # V201634b cnn.com\n",
    "        # V201634c huffingpost.com \n",
    "        # V201634d nytimes.com\n",
    "        # V201634e breitbart.com\n",
    "        # V201634f foxnews.com\n",
    "        # V201634g washingtonpost.com\n",
    "        # V201634h theguardian.com\n",
    "        # V201634i usatoday.com \n",
    "        # V201634j bbc.com \n",
    "        # V201634k npr.org\n",
    "        # V201634m dailycaller.com\n",
    "        # V201634n bloomberg.com\n",
    "        # V201634p buzzfeed.com\n",
    "        # V201634q nbcnews.com\n",
    "\n",
    "\n",
    "        pd.DataFrame(l).to_pickle(filename)\n",
    "\n",
    "    \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "date_list = []\n",
    "\n",
    "start_date = datetime(2020, 6, 1)\n",
    "end_date = datetime(2020, 9, 1)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    date_list.append(current_date.strftime('%Y-%m-%d'))\n",
    "    current_date += delta\n",
    "\n",
    "start_date = datetime(2021, 9, 1)\n",
    "end_date = datetime(2021, 11, 1)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    date_list.append(current_date.strftime('%Y-%m-%d'))\n",
    "    current_date += delta\n",
    "\n",
    "for date in date_list:\n",
    "    fetch_news_for_date(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "4963a043-6b8e-4f8b-b7d8-66a4c27baf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(f\"newsarticles/newsarticles{NEWS_DATE}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "0caf29be-53b6-45b1-bba2-229bee39e56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['huffpost.com', 'yahoo.com', 'edition.cnn.com', 'nytimes.com',\n",
       "       'breitbart.com', 'foxnews.com', 'washingtonpost.com',\n",
       "       'theguardian.com', 'usatoday.com', 'bbc.com', 'npr.org',\n",
       "       'dailycaller.com', 'bloomberg.com', 'buzzfeednews.com',\n",
       "       'nbcnews.com'], dtype=object)"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.source_url.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "7c9dfca3-5b3c-46b9-a332-ea293ec6a0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: newsarticles/newsarticles2020-07-20.pkl\n",
      "Processing: newsarticles/newsarticles2020-05-20.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-03.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-17.pkl\n",
      "Processing: newsarticles/newsarticles2021-09-30.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-16.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-02.pkl\n",
      "Processing: newsarticles/newsarticles2021-10-05.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-14.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-01.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-15.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-11.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-05.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-04.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-10.pkl\n",
      "Processing: newsarticles/newsarticles2020-08-01.pkl\n",
      "Processing: newsarticles/newsarticles2021-10-15.pkl\n",
      "Processing: newsarticles/newsarticles2021-09-20.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-06.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-12.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-13.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-07.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-22.pkl\n",
      "Processing: newsarticles/newsarticles2021-09-05.pkl\n",
      "Processing: newsarticles/newsarticles2021-10-30.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-21.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-09.pkl\n",
      "Processing: newsarticles/newsarticles2020-04-30.pkl\n",
      "Processing: newsarticles/newsarticles20211021.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-08.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-20.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-18.pkl\n",
      "Processing: newsarticles/newsarticles2020-06-19.pkl\n",
      "Processing: newsarticles/newsarticles2021-09-15.pkl\n",
      "Processing: newsarticles/newsarticles2020-05-10.pkl\n",
      "Processing: newsarticles/newsarticles2020-07-01.pkl\n",
      "Processing: newsarticles/newsarticles2020-05-05.pkl\n",
      "Processing: newsarticles/newsarticles2021-11-01.pkl\n",
      "Processing: newsarticles/newsarticles2021-11-15.pkl\n",
      "Processing: newsarticles/newsarticles2020-05-15.pkl\n",
      "Processing: newsarticles/newsarticles2020-05-01.pkl\n",
      "Processing: newsarticles/newsarticles2020-07-10.pkl\n"
     ]
    }
   ],
   "source": [
    "#Fix the media name of all the sources\n",
    "import os\n",
    "\n",
    "folder_path = \"newsarticles\"\n",
    "file_extension = \".pkl\"\n",
    "\n",
    "source_to_name = {'huffpost.com': 'HuffPost',\n",
    "'yahoo.com': 'Yahoo News',\n",
    "'edition.cnn.com': 'CNN',\n",
    "'nytimes.com': 'The New York Times',\n",
    "'breitbart.com': 'Breitbart News',\n",
    "'foxnews.com': 'Fox News',\n",
    "'washingtonpost.com': 'The Washington Post',\n",
    "'theguardian.com': 'The Guardian',\n",
    "'usatoday.com': 'USA Today',\n",
    "'bbc.com': 'BBC News',\n",
    "'npr.org': 'NPR',\n",
    "'dailycaller.com': 'The Daily Caller',\n",
    "'bloomberg.com': 'Bloomberg',\n",
    "'buzzfeednews.com': 'BuzzFeed News',\n",
    "'nbcnews.com': 'NBC News'}\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(file_extension):\n",
    "        full_file_path = os.path.join(folder_path, filename)\n",
    "        # Do something with the file, e.g., load it or process it\n",
    "        print(\"Processing:\", full_file_path)\n",
    "        df = pd.read_pickle(full_file_path)\n",
    "        df['media_name'] = [source_to_name[u] for u in df['source_url']]\n",
    "        df.to_pickle(full_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
